<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>LDA and R-MDA - Allen&#39;s Whiteboard</title>
    <meta property="og:title" content="LDA and R-MDA - Allen&#39;s Whiteboard">
    
    <meta name="twitter:card" content="summary">

    
      
    

    
      
      <meta property="description" content="This is a note of Linear Discriminant Analysis (LDA) and an original Regularized Matrix Discriminant Analysis (R-MDA) method proposed by Jie Su et al, 2018. Both methods are suitable for efficient &amp;hellip;">
      <meta property="og:description" content="This is a note of Linear Discriminant Analysis (LDA) and an original Regularized Matrix Discriminant Analysis (R-MDA) method proposed by Jie Su et al, 2018. Both methods are suitable for efficient &amp;hellip;">
      
    

    
    
    
    <meta name="twitter:image" content="https://allenfrostline.com/logo.png">
    
    

    

    
    




    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    

<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
<script src="/js/math-code.js"></script>


<script>
  (function (u, c) {
    var d = document, t = 'script', o = d.createElement(t), s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (c) { o.addEventListener('load', function (e) { c(e); }); }
    s.parentNode.insertBefore(o, s);
  })('//cdn.jsdelivr.net/npm/pangu@4.0.5/dist/browser/pangu.min.js', function () {
    pangu.spacingPage();
  });
</script>



<script async src="/js/center-img.js"></script>


<script>
    window.minimalAnalytics = {
        trackingId: 'G-B4WMGBPB4Z',
        autoTrack: true, 
    };
</script>
<script src="/index_1423847519945263698.js" async></script>
  </head>

  
  <body class="blog">
    <header class="masthead">
      

<h1><a href="/"><img src="/logo.png" alt="allenfrostline" /></a></h1>



      <nav class="menu">
  <ul>
  
  
  <li><a href="/blog/">Blog</a></li>
  
  <li><a href="/vitae/">Vitae</a></li>
  
  <li><a href="/pottery/">Pottery</a></li>
  
  <li><a href="/recipe/">Recipe</a></li>
  
  













  </ul>
</nav>

    </header>

    <article class="main">
      <header class="title">
      
    <h1>LDA and R-MDA</h1>
    

    <hr style="margin-top:-1em">

    <h3 style="margin-top:-2.3em">
    
        

        
            2018-05-29
        
    
    </h3>



      </header>



<p>This is a note of Linear Discriminant Analysis (LDA) and an original Regularized Matrix Discriminant Analysis (R-MDA) method proposed by Jie Su et al, 2018.  Both methods are suitable for efficient multiclass classification, while the latter is a state-of-the-art version of the classical LDA method s.t. data in matrix forms can be classified without destroying the original structure.</p>
<!-- more -->
<h1 id="a-sketch-of-lda">A Sketch of LDA</h1>
<p>The plain idea behind Discriminant Analysis is to find the optimal partition (or projection, for higher-dimensional problems) s.t. entities within the same class are distributed as compactly as possible and entities between classes are distributed as sparsely as possible. To derive closed-form solutions we have various conditions on the covariance matrices of the input data. When we assume covariances <code>$\boldsymbol{\Sigma}_k$</code> are equal for all classes <code>$k\in\{1,2,\ldots,K\}$</code>, we&rsquo;re following the framework of Linear Discriminant Analysis (LDA).</p>
<p><img src="/images/lda_rmda2.png" alt=""></p>
<p>As shown above, when we consider a 2-dimensional binary classification problem, the LDA is equivalently finding the optimal direction vector <code>$\mathbf{w}$</code> s.t. the ratio of <code>$\mathbf{w}^T\mathbf{S}_b\mathbf{w}$</code> (sum of between-class covariances of the projections) and <code>$\mathbf{w}^T\mathbf{S}_w\mathbf{w}$</code> (sum of within-class covariances of the projections) is maximized. Specifically, we define</p>
<p>$$
\mathbf{S}_b = (\boldsymbol{\mu}_0 - \boldsymbol{\mu}_1)^T(\boldsymbol{\mu}_0 - \boldsymbol{\mu}_1)
$$</p>
<p>and</p>
<div>
$$
\mathbf{S}_w = \sum_{\mathbf{x}\in X_0}(\mathbf{x} - \boldsymbol{\mu}_0)^T(\mathbf{x} - \boldsymbol{\mu}_0) + \sum_{\mathbf{x}\in X_1}(\mathbf{x} - \boldsymbol{\mu}_1)^T(\mathbf{x} - \boldsymbol{\mu}_1).
$$
</div>
<p>Therefore, the objective of this maximization problem is</p>
<p>$$
J = \frac{\mathbf{w}^T\mathbf{S}_b\mathbf{w}}{\mathbf{w}^T\mathbf{S}_w\mathbf{w}}
$$</p>
<p>which is also called the generalized Rayleigh quotiet.</p>
<p>The homogenous objective can be equivalently written into</p>
<p>$$
\begin{align}
\min_{\mathbf{w}}\quad &amp;-\mathbf{w}^T\mathbf{S}_b\mathbf{w}\\
\text{s.t.}\quad &amp;\mathbf{w}^T\mathbf{S}_w\mathbf{w} = 1
\end{align}
$$</p>
<p>which, by using the method of Langrange multipliers, gives solution</p>
<p>$$
\mathbf{w} = \mathbf{S}_w^{-1}(\boldsymbol{\mu}_0 - \boldsymbol{\mu}_1)
$$</p>
<p>and the final prediction for new data <code>$\mathbf{x}$</code> is based on the scale of <code>$\mathbf{w}^T\mathbf{x}$</code>.</p>
<p>For multiclass classification, the solution is similar. Here we propose the score function below without derivation:</p>
<p>$$
\delta_k = \mathbf{x}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_k - \frac{1}{2}\boldsymbol{\mu}_k^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_k + \log\pi_k
$$</p>
<p>where <code>$\boldsymbol{\mu}_k$</code> is the sample mean of all data within class <code>$k$</code>, and <code>$\pi_k$</code> is the percentage of all data that is of this class. By comparing these <code>$k$</code> scores we determine the best prediction with the highest value.</p>
<h1 id="codes-of-lda">Codes of LDA</h1>
<p>We first load necessary packages.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="font-weight:bold">%</span>config InlineBackend<span style="font-weight:bold">.</span>figure_format <span style="font-weight:bold">=</span> <span style="color:#b84">&#39;retina&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="font-weight:bold">import</span> <span style="color:#555">warnings</span>
</span></span><span style="display:flex;"><span><span style="font-weight:bold">import</span> <span style="color:#555">numpy</span> <span style="font-weight:bold">as</span> <span style="color:#555">np</span>
</span></span><span style="display:flex;"><span><span style="font-weight:bold">import</span> <span style="color:#555">matplotlib.pyplot</span> <span style="font-weight:bold">as</span> <span style="color:#555">plt</span>
</span></span><span style="display:flex;"><span><span style="font-weight:bold">from</span> <span style="color:#555">matplotlib</span> <span style="font-weight:bold">import</span> rcParams, rc
</span></span><span style="display:flex;"><span><span style="font-weight:bold">from</span> <span style="color:#555">scipy.optimize</span> <span style="font-weight:bold">import</span> minimize
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>warnings<span style="font-weight:bold">.</span>simplefilter(<span style="color:#b84">&#39;ignore&#39;</span>)
</span></span><span style="display:flex;"><span>rcParams[<span style="color:#b84">&#39;pdf.fonttype&#39;</span>] <span style="font-weight:bold">=</span> <span style="color:#099">42</span>
</span></span><span style="display:flex;"><span>rc(<span style="color:#b84">&#34;font&#34;</span>, <span style="font-weight:bold">**</span>{<span style="color:#b84">&#39;family&#39;</span>: <span style="color:#b84">&#39;serif&#39;</span>, <span style="color:#b84">&#39;serif&#39;</span>: [<span style="color:#b84">&#39;Palatino&#39;</span>], <span style="color:#b84">&#39;size&#39;</span>:<span style="color:#099">13</span>})
</span></span><span style="display:flex;"><span>rc(<span style="color:#b84">&#34;text&#34;</span>, usetex <span style="font-weight:bold">=</span> <span style="font-weight:bold">True</span>)
</span></span><span style="display:flex;"><span>rc(<span style="color:#b84">&#39;legend&#39;</span>, <span style="font-weight:bold">**</span>{<span style="color:#b84">&#39;frameon&#39;</span>: <span style="font-weight:bold">False</span>, <span style="color:#b84">&#39;loc&#39;</span>: <span style="color:#b84">&#39;upper right&#39;</span>, <span style="color:#b84">&#39;fontsize&#39;</span>: <span style="color:#099">15</span>})
</span></span><span style="display:flex;"><span>colors <span style="font-weight:bold">=</span> [<span style="color:#b84">&#39;#b80b0b&#39;</span>, <span style="color:#b84">&#39;#b89a0b&#39;</span>, <span style="color:#b84">&#39;#378000&#39;</span>, <span style="color:#b84">&#39;#2e157e&#39;</span>]
</span></span></code></pre></div><p>Now we define a new class called <code>LDA</code> with a <code>predict</code> (in fact also <code>predict_prob</code>) method.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="font-weight:bold">class</span> <span style="color:#458;font-weight:bold">LDA</span>:
</span></span><span style="display:flex;"><span>    <span style="font-weight:bold">def</span> __init__(self, X, y):  <span style="color:#998;font-style:italic"># X is 2D, y is 1D</span>
</span></span><span style="display:flex;"><span>        <span style="font-weight:bold">assert</span> <span style="color:#999">len</span>(X) <span style="font-weight:bold">==</span> <span style="color:#999">len</span>(y), <span style="color:#b84">&#39;X and y should have the same lengths.&#39;</span>
</span></span><span style="display:flex;"><span>        n_features <span style="font-weight:bold">=</span> <span style="color:#999">len</span>(X[<span style="color:#099">0</span>])
</span></span><span style="display:flex;"><span>        classes <span style="font-weight:bold">=</span> <span style="color:#999">list</span>(<span style="color:#999">set</span>(y))
</span></span><span style="display:flex;"><span>        labels <span style="font-weight:bold">=</span> {c: [] <span style="font-weight:bold">for</span> c <span style="font-weight:bold">in</span> classes}
</span></span><span style="display:flex;"><span>        <span style="font-weight:bold">for</span> _X, _y <span style="font-weight:bold">in</span> <span style="color:#999">zip</span>(X, y):
</span></span><span style="display:flex;"><span>            labels[_y]<span style="font-weight:bold">.</span>append(_X)
</span></span><span style="display:flex;"><span>        labels <span style="font-weight:bold">=</span> {c: np<span style="font-weight:bold">.</span>array(labels[c])<span style="font-weight:bold">.</span>reshape(<span style="font-weight:bold">-</span><span style="color:#099">1</span>, n_features) \
</span></span><span style="display:flex;"><span>                  <span style="font-weight:bold">for</span> c <span style="font-weight:bold">in</span> classes}
</span></span><span style="display:flex;"><span>        pi <span style="font-weight:bold">=</span> {c: <span style="color:#999">len</span>(labels[c]) <span style="font-weight:bold">for</span> c <span style="font-weight:bold">in</span> classes}
</span></span><span style="display:flex;"><span>        mu <span style="font-weight:bold">=</span> {c: labels[c]<span style="font-weight:bold">.</span>sum(axis<span style="font-weight:bold">=</span><span style="color:#099">0</span>) <span style="font-weight:bold">/</span> pi[c] <span style="font-weight:bold">for</span> c <span style="font-weight:bold">in</span> classes}
</span></span><span style="display:flex;"><span>        Sigma <span style="font-weight:bold">=</span> <span style="color:#999">sum</span>(np<span style="font-weight:bold">.</span>cov(labels[c], rowvar<span style="font-weight:bold">=</span><span style="font-weight:bold">False</span>) <span style="font-weight:bold">for</span> c <span style="font-weight:bold">in</span> classes)
</span></span><span style="display:flex;"><span>        inv_Sigma <span style="font-weight:bold">=</span> np<span style="font-weight:bold">.</span>linalg<span style="font-weight:bold">.</span>inv(Sigma)
</span></span><span style="display:flex;"><span>        self<span style="font-weight:bold">.</span>predict_prob <span style="font-weight:bold">=</span> <span style="font-weight:bold">lambda</span> x: {c: np<span style="font-weight:bold">.</span>array(x) <span style="font-weight:bold">@</span> inv_Sigma <span style="font-weight:bold">@</span> mu[c]<span style="font-weight:bold">.</span>T <span style="font-weight:bold">-</span> \
</span></span><span style="display:flex;"><span>                                          mu[c] <span style="font-weight:bold">@</span> inv_Sigma <span style="font-weight:bold">@</span> mu[c]<span style="font-weight:bold">.</span>T <span style="font-weight:bold">/</span> <span style="color:#099">2</span> <span style="font-weight:bold">+</span> \
</span></span><span style="display:flex;"><span>                                          np<span style="font-weight:bold">.</span>log(pi[c])
</span></span><span style="display:flex;"><span>                                       <span style="font-weight:bold">for</span> c <span style="font-weight:bold">in</span> classes}
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="font-weight:bold">def</span> <span style="color:#900;font-weight:bold">predict</span>(self, x):
</span></span><span style="display:flex;"><span>        prob <span style="font-weight:bold">=</span> self<span style="font-weight:bold">.</span>predict_prob(x)
</span></span><span style="display:flex;"><span>        <span style="font-weight:bold">return</span> <span style="color:#999">max</span>(prob, key<span style="font-weight:bold">=</span>prob<span style="font-weight:bold">.</span>get)
</span></span></code></pre></div><p>Then we define three classes of 2D input <code>$\mathbf{X}$</code> and pass it to the classifier. Original as well as the predicted distributions are plotted with accuracy printed below.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>np<span style="font-weight:bold">.</span>random<span style="font-weight:bold">.</span>seed(<span style="color:#099">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>N <span style="font-weight:bold">=</span> <span style="color:#099">100</span>
</span></span><span style="display:flex;"><span>X <span style="font-weight:bold">=</span> np<span style="font-weight:bold">.</span>vstack([np<span style="font-weight:bold">.</span>random<span style="font-weight:bold">.</span>normal(loc<span style="font-weight:bold">=</span>(<span style="color:#099">0</span>,<span style="color:#099">10</span>), scale<span style="font-weight:bold">=</span><span style="color:#099">5</span>, size<span style="font-weight:bold">=</span>(N,<span style="color:#099">2</span>)),
</span></span><span style="display:flex;"><span>               np<span style="font-weight:bold">.</span>random<span style="font-weight:bold">.</span>normal(loc<span style="font-weight:bold">=</span>(<span style="color:#099">10</span>,<span style="font-weight:bold">-</span><span style="color:#099">8</span>), scale<span style="font-weight:bold">=</span><span style="color:#099">5</span>, size<span style="font-weight:bold">=</span>(N,<span style="color:#099">2</span>)),
</span></span><span style="display:flex;"><span>               np<span style="font-weight:bold">.</span>random<span style="font-weight:bold">.</span>normal(loc<span style="font-weight:bold">=</span>(<span style="font-weight:bold">-</span><span style="color:#099">10</span>,<span style="font-weight:bold">-</span><span style="color:#099">8</span>), scale<span style="font-weight:bold">=</span><span style="color:#099">5</span>, size<span style="font-weight:bold">=</span>(N,<span style="color:#099">2</span>))])
</span></span><span style="display:flex;"><span>y <span style="font-weight:bold">=</span> [<span style="color:#099">0</span>] <span style="font-weight:bold">*</span> N <span style="font-weight:bold">+</span> [<span style="color:#099">1</span>] <span style="font-weight:bold">*</span> N <span style="font-weight:bold">+</span> [<span style="color:#099">2</span>] <span style="font-weight:bold">*</span> N
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fig <span style="font-weight:bold">=</span> plt<span style="font-weight:bold">.</span>figure(figsize<span style="font-weight:bold">=</span>(<span style="color:#099">14</span>, <span style="color:#099">7</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>ax <span style="font-weight:bold">=</span> fig<span style="font-weight:bold">.</span>add_subplot(<span style="color:#099">121</span>)
</span></span><span style="display:flex;"><span><span style="font-weight:bold">for</span> _X, _y <span style="font-weight:bold">in</span> <span style="color:#999">zip</span>(X, y):
</span></span><span style="display:flex;"><span>    ax<span style="font-weight:bold">.</span>scatter(_X[<span style="color:#099">0</span>], _X[<span style="color:#099">1</span>], c<span style="font-weight:bold">=</span>colors[_y], s<span style="font-weight:bold">=</span><span style="color:#099">50</span>,
</span></span><span style="display:flex;"><span>               alpha<span style="font-weight:bold">=</span><span style="color:#099">.5</span>, edgecolor<span style="font-weight:bold">=</span><span style="color:#b84">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>ax<span style="font-weight:bold">.</span>set_xlim(<span style="font-weight:bold">-</span><span style="color:#099">25</span>, <span style="color:#099">25</span>)
</span></span><span style="display:flex;"><span>ax<span style="font-weight:bold">.</span>set_ylim(<span style="font-weight:bold">-</span><span style="color:#099">25</span>, <span style="color:#099">25</span>)
</span></span><span style="display:flex;"><span>ax<span style="font-weight:bold">.</span>set_title(<span style="color:#b84">&#39;Original Classes&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cls <span style="font-weight:bold">=</span> LDA(X, y)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>ax <span style="font-weight:bold">=</span> fig<span style="font-weight:bold">.</span>add_subplot(<span style="color:#099">122</span>)
</span></span><span style="display:flex;"><span>correct <span style="font-weight:bold">=</span> <span style="color:#099">0</span>
</span></span><span style="display:flex;"><span><span style="font-weight:bold">for</span> _X, _y <span style="font-weight:bold">in</span> <span style="color:#999">zip</span>(X, y):
</span></span><span style="display:flex;"><span>    _y_pred <span style="font-weight:bold">=</span> cls<span style="font-weight:bold">.</span>predict(_X)
</span></span><span style="display:flex;"><span>    correct <span style="font-weight:bold">+=</span> (_y <span style="font-weight:bold">==</span> _y_pred)
</span></span><span style="display:flex;"><span>    ax<span style="font-weight:bold">.</span>scatter(_X[<span style="color:#099">0</span>], _X[<span style="color:#099">1</span>], c<span style="font-weight:bold">=</span>colors[_y_pred], s<span style="font-weight:bold">=</span><span style="color:#099">50</span>,
</span></span><span style="display:flex;"><span>               alpha<span style="font-weight:bold">=</span><span style="color:#099">.5</span>, edgecolor<span style="font-weight:bold">=</span><span style="color:#b84">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>ax<span style="font-weight:bold">.</span>set_xlim(<span style="font-weight:bold">-</span><span style="color:#099">25</span>, <span style="color:#099">25</span>)
</span></span><span style="display:flex;"><span>ax<span style="font-weight:bold">.</span>set_ylim(<span style="font-weight:bold">-</span><span style="color:#099">25</span>, <span style="color:#099">25</span>)
</span></span><span style="display:flex;"><span>ax<span style="font-weight:bold">.</span>set_title(<span style="color:#b84">&#39;Predicted Classes&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="font-weight:bold">.</span>show()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>accuracy <span style="font-weight:bold">=</span> correct <span style="font-weight:bold">/</span> (<span style="color:#099">3</span> <span style="font-weight:bold">*</span> N)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(<span style="color:#b84">&#39;Training accuracy: </span><span style="color:#b84">{:.2f}</span><span style="color:#b84">%&#39;</span><span style="font-weight:bold">.</span>format(accuracy <span style="font-weight:bold">*</span> N))
</span></span></code></pre></div><p><img src="/images/lda_rmda1.png" alt=""></p>
<pre><code>Training accuracy: 95.67%
</code></pre>
<h1 id="a-sketch-of-r-mda">A Sketch of R-MDA</h1>
<p>For data with inherent matrix forms like electroencephalogram (EEG) data introduced in Jie Su (2018), the classical LDA is not the most appropriate solution since it forcibly requires vector input. To use LDA for classification on such datasets we have to vectorize the matrices and potentially losing some critical structural information. Authors of this paper invented this new method called Regularized Matrix Discriminant Analysis (R-MDA) that naturally takes matric input in analysis. Furthermore, noticing that inversing large matrix <code>$\mathbf{S}_w$</code> in high dimensions can be computationally burdonsome, they adopted the Alternating Direction Method of Multipliers (ADMM) to iteratively optimize the objective instead of the widely-used Singular Valur Decomposition (SVD). A graphical representation of the R-MDA compared with LDA is as follows.</p>
<p><img src="/images/lda_rmda3.png" alt=""></p>
<p>The algorithm is implemented below. Notice here I skipped the Gradient Descent (GD) approach in the minimization during iterations and opt for the <code>minimize</code> function in <code>scipy.optimize</code>. I did so to make the structure simpler without hurting the understanding of the whole algorithm. For more detailed illustration please resort to the original paper.</p>
<h1 id="codes-of-r-mda">Codes of R-MDA</h1>
<p>Again we first define the class <code>RMDA</code>. The <code>predict</code> method now takes a matrix.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="font-weight:bold">class</span> <span style="color:#458;font-weight:bold">RMDA</span>:
</span></span><span style="display:flex;"><span>    <span style="font-weight:bold">def</span> __init__(self, X, y, learning_rate<span style="font-weight:bold">=</span><span style="color:#099">0.01</span>, max_iter<span style="font-weight:bold">=</span><span style="color:#099">100</span>, tau<span style="font-weight:bold">=</span><span style="color:#099">0.05</span>, rho<span style="font-weight:bold">=</span><span style="color:#099">0.01</span>):  <span style="color:#998;font-style:italic"># X is 3D, y is 1D</span>
</span></span><span style="display:flex;"><span>        <span style="font-weight:bold">assert</span> <span style="color:#999">len</span>(X) <span style="font-weight:bold">==</span> <span style="color:#999">len</span>(y), <span style="color:#b84">&#39;X and y should have the same lengths.&#39;</span>
</span></span><span style="display:flex;"><span>        shape <span style="font-weight:bold">=</span> X[<span style="color:#099">0</span>]<span style="font-weight:bold">.</span>shape
</span></span><span style="display:flex;"><span>        classes <span style="font-weight:bold">=</span> <span style="color:#999">list</span>(<span style="color:#999">set</span>(y))
</span></span><span style="display:flex;"><span>        labels <span style="font-weight:bold">=</span> {c: [] <span style="font-weight:bold">for</span> c <span style="font-weight:bold">in</span> classes}
</span></span><span style="display:flex;"><span>        <span style="font-weight:bold">for</span> _X, _y <span style="font-weight:bold">in</span> <span style="color:#999">zip</span>(X, y):
</span></span><span style="display:flex;"><span>            labels[_y]<span style="font-weight:bold">.</span>append(_X)
</span></span><span style="display:flex;"><span>        n <span style="font-weight:bold">=</span> {c: <span style="color:#999">len</span>(labels[c]) <span style="font-weight:bold">for</span> c <span style="font-weight:bold">in</span> classes}
</span></span><span style="display:flex;"><span>        N <span style="font-weight:bold">=</span> <span style="color:#999">len</span>(y)
</span></span><span style="display:flex;"><span>        K<span style="font-weight:bold">=</span>  <span style="color:#999">len</span>(classes)
</span></span><span style="display:flex;"><span>        mu <span style="font-weight:bold">=</span> <span style="color:#999">sum</span>(X) <span style="font-weight:bold">/</span> N
</span></span><span style="display:flex;"><span>        X_tilde <span style="font-weight:bold">=</span> [_X <span style="font-weight:bold">-</span> mu <span style="font-weight:bold">for</span> _X <span style="font-weight:bold">in</span> X]
</span></span><span style="display:flex;"><span>        Y_tilde <span style="font-weight:bold">=</span> np<span style="font-weight:bold">.</span>zeros((N, K))
</span></span><span style="display:flex;"><span>        <span style="font-weight:bold">for</span> i <span style="font-weight:bold">in</span> <span style="color:#999">range</span>(N):
</span></span><span style="display:flex;"><span>            <span style="font-weight:bold">for</span> j <span style="font-weight:bold">in</span> <span style="color:#999">range</span>(K):
</span></span><span style="display:flex;"><span>                <span style="font-weight:bold">if</span> y[i] <span style="font-weight:bold">==</span> classes[j]:
</span></span><span style="display:flex;"><span>                    Y_tilde[i,j] <span style="font-weight:bold">=</span> np<span style="font-weight:bold">.</span>sqrt(N <span style="font-weight:bold">/</span> n[j]) <span style="font-weight:bold">-</span> np<span style="font-weight:bold">.</span>sqrt(n[j] <span style="font-weight:bold">/</span> N)
</span></span><span style="display:flex;"><span>                <span style="font-weight:bold">else</span>:
</span></span><span style="display:flex;"><span>                    Y_tilde[i,j] <span style="font-weight:bold">=</span> <span style="font-weight:bold">-</span>np<span style="font-weight:bold">.</span>sqrt(n[j] <span style="font-weight:bold">/</span> N)
</span></span><span style="display:flex;"><span>        W <span style="font-weight:bold">=</span> [np<span style="font-weight:bold">.</span>random<span style="font-weight:bold">.</span>normal(size<span style="font-weight:bold">=</span>shape[::<span style="font-weight:bold">-</span><span style="color:#099">1</span>]) <span style="font-weight:bold">for</span> j <span style="font-weight:bold">in</span> <span style="color:#999">range</span>(K)]
</span></span><span style="display:flex;"><span>        L <span style="font-weight:bold">=</span> <span style="font-weight:bold">lambda</span> W: <span style="color:#999">sum</span>((np<span style="font-weight:bold">.</span>trace(X_tilde[i]<span style="font-weight:bold">.</span>dot(W[j])) <span style="font-weight:bold">-</span> Y_tilde[i,j])<span style="font-weight:bold">**</span><span style="color:#099">2</span> \
</span></span><span style="display:flex;"><span>                          <span style="font-weight:bold">for</span> i <span style="font-weight:bold">in</span> <span style="color:#999">range</span>(N) <span style="font-weight:bold">for</span> j <span style="font-weight:bold">in</span> <span style="color:#999">range</span>(K)) <span style="font-weight:bold">/</span> (<span style="color:#099">2</span> <span style="font-weight:bold">*</span> N) <span style="font-weight:bold">+</span> \
</span></span><span style="display:flex;"><span>                      <span style="color:#999">sum</span>(self<span style="font-weight:bold">.</span>nuclear_norm(W[j]) <span style="font-weight:bold">for</span> j <span style="font-weight:bold">in</span> <span style="color:#999">range</span>(K)) <span style="font-weight:bold">*</span> tau
</span></span><span style="display:flex;"><span>        S <span style="font-weight:bold">=</span> [np<span style="font-weight:bold">.</span>random<span style="font-weight:bold">.</span>normal(size<span style="font-weight:bold">=</span>shape) <span style="font-weight:bold">for</span> j <span style="font-weight:bold">in</span> <span style="color:#999">range</span>(K)]
</span></span><span style="display:flex;"><span>        V <span style="font-weight:bold">=</span> [np<span style="font-weight:bold">.</span>random<span style="font-weight:bold">.</span>normal(size<span style="font-weight:bold">=</span>shape) <span style="font-weight:bold">for</span> j <span style="font-weight:bold">in</span> <span style="color:#999">range</span>(K)]
</span></span><span style="display:flex;"><span>        L_temp <span style="font-weight:bold">=</span> <span style="color:#099">0</span>
</span></span><span style="display:flex;"><span>        <span style="font-weight:bold">for</span> iteration <span style="font-weight:bold">in</span> <span style="color:#999">range</span>(max_iter):
</span></span><span style="display:flex;"><span>            <span style="font-weight:bold">for</span> j <span style="font-weight:bold">in</span> <span style="color:#999">range</span>(K):
</span></span><span style="display:flex;"><span>                L1 <span style="font-weight:bold">=</span> <span style="font-weight:bold">lambda</span> w: <span style="color:#999">sum</span>((np<span style="font-weight:bold">.</span>trace(X_tilde[i]<span style="font-weight:bold">.</span>dot(w)) <span style="font-weight:bold">-</span> Y_tilde[i,j])<span style="font-weight:bold">**</span><span style="color:#099">2</span> \
</span></span><span style="display:flex;"><span>                                   <span style="font-weight:bold">for</span> i <span style="font-weight:bold">in</span> <span style="color:#999">range</span>(N)) <span style="font-weight:bold">/</span> (<span style="color:#099">2</span> <span style="font-weight:bold">*</span> N) <span style="font-weight:bold">-</span> \
</span></span><span style="display:flex;"><span>                               np<span style="font-weight:bold">.</span>trace(V[j]<span style="font-weight:bold">.</span>dot(w)) <span style="font-weight:bold">+</span> \
</span></span><span style="display:flex;"><span>                               self<span style="font-weight:bold">.</span>frobenius_norm(S[j] <span style="font-weight:bold">-</span> w)<span style="font-weight:bold">**</span><span style="color:#099">2</span> <span style="font-weight:bold">*</span> rho <span style="font-weight:bold">/</span> <span style="color:#099">2</span>
</span></span><span style="display:flex;"><span>                L1_ <span style="font-weight:bold">=</span> <span style="font-weight:bold">lambda</span> w: L1(w<span style="font-weight:bold">.</span>reshape(shape))
</span></span><span style="display:flex;"><span>                W[j] <span style="font-weight:bold">=</span> minimize(fun<span style="font-weight:bold">=</span>L1_, x0<span style="font-weight:bold">=</span>W[j])<span style="font-weight:bold">.</span>x<span style="font-weight:bold">.</span>reshape(shape)
</span></span><span style="display:flex;"><span>                L2 <span style="font-weight:bold">=</span> <span style="font-weight:bold">lambda</span> s: self<span style="font-weight:bold">.</span>nuclear_norm(s) <span style="font-weight:bold">*</span> tau <span style="font-weight:bold">+</span> \
</span></span><span style="display:flex;"><span>                               np<span style="font-weight:bold">.</span>trace(V[j]<span style="font-weight:bold">.</span>dot(s)) <span style="font-weight:bold">+</span> \
</span></span><span style="display:flex;"><span>                               self<span style="font-weight:bold">.</span>frobenius_norm(S[j] <span style="font-weight:bold">-</span> w)<span style="font-weight:bold">**</span><span style="color:#099">2</span> <span style="font-weight:bold">*</span> rho <span style="font-weight:bold">/</span> <span style="color:#099">2</span>
</span></span><span style="display:flex;"><span>                L2_ <span style="font-weight:bold">=</span> <span style="font-weight:bold">lambda</span> s: L2(s<span style="font-weight:bold">.</span>reshape(shape))
</span></span><span style="display:flex;"><span>                S[j] <span style="font-weight:bold">=</span> minimize(fun<span style="font-weight:bold">=</span>L2_, x0<span style="font-weight:bold">=</span>S[j])<span style="font-weight:bold">.</span>x<span style="font-weight:bold">.</span>reshape(shape)
</span></span><span style="display:flex;"><span>                V[j] <span style="font-weight:bold">-=</span> (W[j] <span style="font-weight:bold">-</span> S[j]) <span style="font-weight:bold">*</span> rho
</span></span><span style="display:flex;"><span>            L_W <span style="font-weight:bold">=</span> L(W)
</span></span><span style="display:flex;"><span>            dL <span style="font-weight:bold">=</span> <span style="color:#999">abs</span>(L_W <span style="font-weight:bold">-</span> L_temp)
</span></span><span style="display:flex;"><span>            <span style="color:#999">print</span>(<span style="color:#b84">&#39;[</span><span style="color:#b84">{}</span><span style="color:#b84">/</span><span style="color:#b84">{}</span><span style="color:#b84">] </span><span style="color:#b84">{:&lt;10.4f}</span><span style="color:#b84">&#39;</span><span style="font-weight:bold">.</span>format(iteration <span style="font-weight:bold">+</span> <span style="color:#099">1</span>, max_iter, dL), end<span style="font-weight:bold">=</span><span style="color:#b84">&#39;</span><span style="color:#b84">\r</span><span style="color:#b84">&#39;</span>)
</span></span><span style="display:flex;"><span>            L_temp <span style="font-weight:bold">=</span> L_W
</span></span><span style="display:flex;"><span>            <span style="font-weight:bold">if</span> dL <span style="font-weight:bold">&lt;</span> <span style="color:#099">1e-9</span>: <span style="font-weight:bold">break</span>
</span></span><span style="display:flex;"><span>        <span style="font-weight:bold">if</span> iteration <span style="font-weight:bold">==</span> max_iter <span style="font-weight:bold">-</span> <span style="color:#099">1</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#999">print</span>(<span style="color:#b84">&#39;Optimization failed to converge.&#39;</span>)
</span></span><span style="display:flex;"><span>        <span style="font-weight:bold">else</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#999">print</span>(<span style="color:#b84">&#39;Optimization converged successfully.&#39;</span>)
</span></span><span style="display:flex;"><span>        self<span style="font-weight:bold">.</span>predict_prob <span style="font-weight:bold">=</span> <span style="font-weight:bold">lambda</span> X: {classes[j]: np<span style="font-weight:bold">.</span>trace(X<span style="font-weight:bold">.</span>dot(W[j])) <span style="font-weight:bold">for</span> j <span style="font-weight:bold">in</span> <span style="color:#999">range</span>(K)}
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="font-weight:bold">def</span> <span style="color:#900;font-weight:bold">nuclear_norm</span>(self, X):
</span></span><span style="display:flex;"><span>        <span style="font-weight:bold">return</span> np<span style="font-weight:bold">.</span>linalg<span style="font-weight:bold">.</span>svd(X)[<span style="color:#099">1</span>]<span style="font-weight:bold">.</span>sum()
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="font-weight:bold">def</span> <span style="color:#900;font-weight:bold">frobenius_norm</span>(self, X):
</span></span><span style="display:flex;"><span>        <span style="font-weight:bold">return</span> <span style="color:#999">sum</span>(X[i,j]<span style="font-weight:bold">**</span><span style="color:#099">2</span> <span style="font-weight:bold">for</span> i <span style="font-weight:bold">in</span> <span style="color:#999">range</span>(X<span style="font-weight:bold">.</span>shape[<span style="color:#099">0</span>]) <span style="font-weight:bold">for</span> j <span style="font-weight:bold">in</span> <span style="color:#999">range</span>(X<span style="font-weight:bold">.</span>shape[<span style="color:#099">1</span>]))
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="font-weight:bold">def</span> <span style="color:#900;font-weight:bold">predict</span>(self, X):
</span></span><span style="display:flex;"><span>        prob <span style="font-weight:bold">=</span> self<span style="font-weight:bold">.</span>predict_prob(X)
</span></span><span style="display:flex;"><span>        <span style="font-weight:bold">return</span> <span style="color:#999">max</span>(prob, key<span style="font-weight:bold">=</span>prob<span style="font-weight:bold">.</span>get)
</span></span></code></pre></div><p>Then we train the model and print the final accuracy.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>np<span style="font-weight:bold">.</span>random<span style="font-weight:bold">.</span>seed(<span style="color:#099">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>N <span style="font-weight:bold">=</span> <span style="color:#099">100</span>
</span></span><span style="display:flex;"><span>X <span style="font-weight:bold">=</span> np<span style="font-weight:bold">.</span>vstack([np<span style="font-weight:bold">.</span>random<span style="font-weight:bold">.</span>normal(loc<span style="font-weight:bold">=</span>((<span style="color:#099">10</span>,<span style="color:#099">0</span>),(<span style="color:#099">0</span>,<span style="color:#099">10</span>)), scale<span style="font-weight:bold">=</span><span style="color:#099">5</span>, size<span style="font-weight:bold">=</span>(N,<span style="color:#099">2</span>,<span style="color:#099">2</span>))] <span style="font-weight:bold">+</span> \
</span></span><span style="display:flex;"><span>              [np<span style="font-weight:bold">.</span>random<span style="font-weight:bold">.</span>normal(loc<span style="font-weight:bold">=</span>((<span style="color:#099">0</span>,<span style="color:#099">10</span>),(<span style="color:#099">10</span>,<span style="color:#099">0</span>)), scale<span style="font-weight:bold">=</span><span style="color:#099">5</span>, size<span style="font-weight:bold">=</span>(N,<span style="color:#099">2</span>,<span style="color:#099">2</span>))] <span style="font-weight:bold">+</span> \
</span></span><span style="display:flex;"><span>              [np<span style="font-weight:bold">.</span>random<span style="font-weight:bold">.</span>normal(loc<span style="font-weight:bold">=</span>((<span style="font-weight:bold">-</span><span style="color:#099">8</span>,<span style="font-weight:bold">-</span><span style="color:#099">8</span>),(<span style="font-weight:bold">-</span><span style="color:#099">8</span>,<span style="color:#099">8</span>)), scale<span style="font-weight:bold">=</span><span style="color:#099">5</span>, size<span style="font-weight:bold">=</span>(N,<span style="color:#099">2</span>,<span style="color:#099">2</span>))])
</span></span><span style="display:flex;"><span>y <span style="font-weight:bold">=</span> [<span style="color:#099">0</span>] <span style="font-weight:bold">*</span> N <span style="font-weight:bold">+</span> [<span style="color:#099">1</span>] <span style="font-weight:bold">*</span> N <span style="font-weight:bold">+</span> [<span style="color:#099">2</span>] <span style="font-weight:bold">*</span> N
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cls <span style="font-weight:bold">=</span> RMDA(X, y)
</span></span><span style="display:flex;"><span>correct <span style="font-weight:bold">=</span> <span style="color:#099">0</span>
</span></span><span style="display:flex;"><span><span style="font-weight:bold">for</span> _X, _y <span style="font-weight:bold">in</span> <span style="color:#999">zip</span>(X, y):
</span></span><span style="display:flex;"><span>    _y_pred <span style="font-weight:bold">=</span> cls<span style="font-weight:bold">.</span>predict(_X)
</span></span><span style="display:flex;"><span>    correct <span style="font-weight:bold">+=</span> (_y <span style="font-weight:bold">==</span> _y_pred)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>accuracy <span style="font-weight:bold">=</span> correct <span style="font-weight:bold">/</span> (<span style="color:#099">3</span> <span style="font-weight:bold">*</span> N)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(<span style="color:#b84">&#39;Training accuracy: </span><span style="color:#b84">{:.2f}</span><span style="color:#b84">%&#39;</span><span style="font-weight:bold">.</span>format(accuracy <span style="font-weight:bold">*</span> <span style="color:#099">100</span>))
</span></span></code></pre></div><pre><code>Optimization converged successfully.
Training accuracy: 87.00%
</code></pre>
<p>Further analysis and debugging should be expected. Any correction in comments is also welcomed. ðŸ˜‡</p>
<hr>
<h1 id="references">References</h1>
<ul>
<li>Su, Jie, Linbo Qing, Xiaohai He, Hang Zhang, Jing Zhou, and Yonghong Peng. &ldquo;A New Regularized Matrix Discriminant Analysis (R-MDA) Enabled Human-Centered EEG Monitoring Systems.&rdquo; IEEE Access 6 (2018): 13911-13920.</li>
<li>Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning. Vol. 1. New York: Springer series in statistics, 2001.</li>
</ul>


<script>
  var unfocusableElems = document.querySelectorAll('pre');
  unfocusableElems.forEach(function (el) { el.setAttribute("tabindex", "-1"); });
  var unfocusableElems = document.querySelectorAll('iframe');
  unfocusableElems.forEach(function (el) { el.setAttribute("tabindex", "-1"); });
</script>

<footer>
  
<br><br>
<nav class="post-nav">
  <span class="nav-prev">&larr; <a href="/blog/optimal-order-execution-5/">Literature Review on Optimal Order Execution (5)</a></span>
  <span class="nav-next"><a href="/blog/giethorn/">A Photo Taken at Giethoorn</a> &rarr;</span>
</nav>

<script type="text/javascript">
document.addEventListener('keyup', function(e) {
  if (e.target.nodeName.toUpperCase() != 'BODY') return;
  var url = false;
  if (e.which == 37) {  
    
    url = '\/blog\/optimal-order-execution-5\/';
    
  } else if (e.which == 39) {  
    
    url = '\/blog\/giethorn\/';
    
  }
  if (url) window.location = url;
});
</script>



<script src="https://giscus.app/client.js" data-repo="allenfrostline/allenfrostline.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3NzEzOTkxNg==" data-category="General" data-category-id="DIC_kwDOBJkPzM4CbgIQ"
        data-mapping="pathname" data-strict="0" data-reactions-enabled="0" data-emit-metadata="0"
        data-input-position="bottom" data-theme="light" data-lang="en" data-loading="lazy" crossorigin="anonymous"
        async>
        </script>



<script async src="/js/alt-title.js"></script>

<script async src="/js/center-img.js"></script>

<script async src="/js/external-link.js"></script>

<script async src="/js/fix-footnote.js"></script>

<script async src="/js/header-link.js"></script>

<script async src="/js/load-typekit.js"></script>

<script async src="/js/math-code.js"></script>

<script async src="/js/mermaid.min.js"></script>

<script async src="/js/right-quote.js"></script>


<script src="/js/math-code.js"></script>

  
  
  
  
</footer>
</article>
</body>

</html>
