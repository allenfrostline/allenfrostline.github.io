<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>Ensemble Modeling in a Binary Classification Problem in Chinese A-Share Market - Allen&#39;s Whiteboard</title>
    <meta property="og:title" content="Ensemble Modeling in a Binary Classification Problem in Chinese A-Share Market - Allen&#39;s Whiteboard">
    
    <meta name="twitter:card" content="summary">

    
      
    

    
      
      <meta property="description" content="In this research paper we try to use as much information on a stock as we can on Ricequant, to train a robust binary classifier for expected returns on a rolling basis. As an extra, we create a &amp;hellip;">
      <meta property="og:description" content="In this research paper we try to use as much information on a stock as we can on Ricequant, to train a robust binary classifier for expected returns on a rolling basis. As an extra, we create a &amp;hellip;">
      
    

    
    
    <meta name="twitter:image" content="https://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/images/schematic.png">
    
    
    
    

    

    
    




    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    

<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
<script src="/js/math-code.js"></script>


<script>
  (function (u, c) {
    var d = document, t = 'script', o = d.createElement(t), s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (c) { o.addEventListener('load', function (e) { c(e); }); }
    s.parentNode.insertBefore(o, s);
  })('//cdn.jsdelivr.net/npm/pangu@4.0.5/dist/browser/pangu.min.js', function () {
    pangu.spacingPage();
  });
</script>



<script async src="/js/center-img.js"></script>


<script>
    window.minimalAnalytics = {
        trackingId: 'G-B4WMGBPB4Z',
        autoTrack: true, 
    };
</script>
<script src="/index_1423847519945263698.js" async></script>
  </head>

  
  <body class="blog">
    <header class="masthead">
      

<h1><a href="/"><img src="/logo.png" alt="allenfrostline" /></a></h1>



      <nav class="menu">
  <ul>
  
  
  <li><a href="/blog/">Blog</a></li>
  
  <li><a href="/pottery/">Pottery</a></li>
  
  <li><a href="/recipe/">Recipe</a></li>
  
  <li><a href="/kotoba">Kotoba</a></li>
  
  <li><a href="/vitae/">Vitae</a></li>
  
  













  </ul>
</nav>

    </header>

    <article class="main">
      <header class="title">
      
    <h1>Ensemble Modeling in a Binary Classification Problem in Chinese A-Share Market</h1>
    

    <hr style="margin-top:-1em">

    <h3 style="margin-top:-2.3em">
    
        

        
            2017-11-15
        
    
    </h3>



      </header>



<p>In this research paper we try to use as much information on a stock as we can on Ricequant, to train a robust binary classifier for expected returns on a rolling basis. As an extra, we create a brand-new accuracy metric based on behavioral economics for model traing, which enhanced the fitting of the models (in the language of classical metrics, e.g. accuracy or pricision scores) by 3 to 5 times. The advantage of this new metric will be covered in the corresponding sector.</p>
<!-- more -->
<h1 id="section-1-environment-preparation">Section 1: Environment Preparation</h1>
<p>First we import the necessary packages we&rsquo;re going to use later.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="font-weight:bold">%</span>config InlineBackend<span style="font-weight:bold">.</span>figure_format <span style="font-weight:bold">=</span> <span style="color:#b84">&#39;retina&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="font-weight:bold">import</span> <span style="color:#555">numpy</span> <span style="font-weight:bold">as</span> <span style="color:#555">np</span>
</span></span><span style="display:flex;"><span><span style="font-weight:bold">import</span> <span style="color:#555">pandas</span> <span style="font-weight:bold">as</span> <span style="color:#555">pd</span>
</span></span><span style="display:flex;"><span><span style="font-weight:bold">from</span> <span style="color:#555">scipy.special</span> <span style="font-weight:bold">import</span> logit
</span></span><span style="display:flex;"><span><span style="font-weight:bold">from</span> <span style="color:#555">datetime</span> <span style="font-weight:bold">import</span> datetime
</span></span><span style="display:flex;"><span><span style="font-weight:bold">from</span> <span style="color:#555">dateutil.relativedelta</span> <span style="font-weight:bold">import</span> relativedelta
</span></span><span style="display:flex;"><span><span style="font-weight:bold">from</span> <span style="color:#555">tech</span> <span style="font-weight:bold">import</span> technical
</span></span><span style="display:flex;"><span><span style="font-weight:bold">import</span> <span style="color:#555">warnings</span>
</span></span><span style="display:flex;"><span>warnings<span style="font-weight:bold">.</span>filterwarnings(<span style="color:#b84">&#39;ignore&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="font-weight:bold">import</span> <span style="color:#555">matplotlib.pyplot</span> <span style="font-weight:bold">as</span> <span style="color:#555">plt</span>
</span></span><span style="display:flex;"><span><span style="font-weight:bold">import</span> <span style="color:#555">seaborn</span> <span style="font-weight:bold">as</span> <span style="color:#555">sns</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="font-weight:bold">from</span> <span style="color:#555">xgboost</span> <span style="font-weight:bold">import</span> XGBClassifier
</span></span><span style="display:flex;"><span><span style="font-weight:bold">from</span> <span style="color:#555">sklearn.decomposition</span> <span style="font-weight:bold">import</span> PCA, KernelPCA
</span></span><span style="display:flex;"><span><span style="font-weight:bold">from</span> <span style="color:#555">sklearn.cross_validation</span> <span style="font-weight:bold">import</span> KFold, cross_val_score
</span></span><span style="display:flex;"><span><span style="font-weight:bold">from</span> <span style="color:#555">sklearn.metrics</span> <span style="font-weight:bold">import</span> make_scorer, accuracy_score, precision_score, recall_score
</span></span><span style="display:flex;"><span><span style="font-weight:bold">from</span> <span style="color:#555">sklearn.grid_search</span> <span style="font-weight:bold">import</span> GridSearchCV
</span></span><span style="display:flex;"><span><span style="font-weight:bold">from</span> <span style="color:#555">sklearn.feature_selection</span> <span style="font-weight:bold">import</span> VarianceThreshold, RFE, SelectKBest, chi2
</span></span><span style="display:flex;"><span><span style="font-weight:bold">from</span> <span style="color:#555">sklearn.preprocessing</span> <span style="font-weight:bold">import</span> MinMaxScaler
</span></span><span style="display:flex;"><span><span style="font-weight:bold">from</span> <span style="color:#555">sklearn.pipeline</span> <span style="font-weight:bold">import</span> Pipeline, FeatureUnion
</span></span><span style="display:flex;"><span><span style="font-weight:bold">from</span> <span style="color:#555">sklearn.linear_model</span> <span style="font-weight:bold">import</span> LogisticRegression
</span></span><span style="display:flex;"><span><span style="font-weight:bold">from</span> <span style="color:#555">sklearn.discriminant_analysis</span> <span style="font-weight:bold">import</span> LinearDiscriminantAnalysis
</span></span><span style="display:flex;"><span><span style="font-weight:bold">from</span> <span style="color:#555">sklearn.neighbors</span> <span style="font-weight:bold">import</span> KNeighborsClassifier
</span></span><span style="display:flex;"><span><span style="font-weight:bold">from</span> <span style="color:#555">sklearn.tree</span> <span style="font-weight:bold">import</span> DecisionTreeClassifier
</span></span><span style="display:flex;"><span><span style="font-weight:bold">from</span> <span style="color:#555">sklearn.naive_bayes</span> <span style="font-weight:bold">import</span> GaussianNB
</span></span><span style="display:flex;"><span><span style="font-weight:bold">from</span> <span style="color:#555">sklearn.svm</span> <span style="font-weight:bold">import</span> SVC
</span></span><span style="display:flex;"><span><span style="font-weight:bold">from</span> <span style="color:#555">sklearn.ensemble</span> <span style="font-weight:bold">import</span> BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, VotingClassifier, RandomForestClassifier, AdaBoostClassifier
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>sns<span style="font-weight:bold">.</span>set_style(<span style="color:#b84">&#39;whitegrid&#39;</span>)
</span></span><span style="display:flex;"><span>pd<span style="font-weight:bold">.</span>set_option(<span style="color:#b84">&#39;display.max_columns&#39;</span>, <span style="font-weight:bold">None</span>) <span style="color:#998;font-style:italic"># display all columns</span>
</span></span></code></pre></div><p>Global configurations.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>pool <span style="font-weight:bold">=</span> index_components(<span style="color:#b84">&#39;000050.XSHG&#39;</span>)
</span></span><span style="display:flex;"><span>window <span style="font-weight:bold">=</span> <span style="color:#099">3</span>  <span style="color:#998;font-style:italic"># use the past 6 months for training and testing</span>
</span></span><span style="display:flex;"><span>risk_preference <span style="font-weight:bold">=</span> <span style="font-weight:bold">-</span><span style="color:#099">0.2</span>  <span style="color:#998;font-style:italic"># in the range of (-1,1) exclusive!</span>
</span></span><span style="display:flex;"><span>verbose <span style="font-weight:bold">=</span> <span style="font-weight:bold">True</span>  <span style="color:#998;font-style:italic"># set to False when you don&#39;t want to waste time on plotting</span>
</span></span></code></pre></div><h1 id="section-2-data-preparation">Section 2: Data Preparation</h1>
<p>Load the raw data and encapsulate into a Pandas panel.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>today <span style="font-weight:bold">=</span> datetime<span style="font-weight:bold">.</span>today()
</span></span><span style="display:flex;"><span>s <span style="font-weight:bold">=</span> pool[<span style="color:#099">11</span>]
</span></span><span style="display:flex;"><span>price_df <span style="font-weight:bold">=</span> get_price(s,today<span style="font-weight:bold">-</span>relativedelta(months<span style="font-weight:bold">=</span>window<span style="font-weight:bold">+</span><span style="color:#099">6</span>),today<span style="font-weight:bold">-</span>relativedelta(days<span style="font-weight:bold">=</span><span style="color:#099">1</span>),<span style="color:#b84">&#39;1d&#39;</span>,[<span style="color:#b84">&#39;open&#39;</span>,<span style="color:#b84">&#39;close&#39;</span>,<span style="color:#b84">&#39;high&#39;</span>,<span style="color:#b84">&#39;low&#39;</span>,<span style="color:#b84">&#39;volume&#39;</span>])
</span></span><span style="display:flex;"><span>price_df[<span style="color:#b84">&#39;mkt&#39;</span>] <span style="font-weight:bold">=</span> get_price(<span style="color:#b84">&#39;000300.XSHG&#39;</span>,today<span style="font-weight:bold">-</span>relativedelta(months<span style="font-weight:bold">=</span>window<span style="font-weight:bold">+</span><span style="color:#099">6</span>),today<span style="font-weight:bold">-</span>relativedelta(days<span style="font-weight:bold">=</span><span style="color:#099">1</span>),<span style="color:#b84">&#39;1d&#39;</span>,[<span style="color:#b84">&#39;close&#39;</span>])
</span></span><span style="display:flex;"><span>X_ <span style="font-weight:bold">=</span> pd<span style="font-weight:bold">.</span>DataFrame(technical(price_df),index<span style="font-weight:bold">=</span>price_df<span style="font-weight:bold">.</span>index,columns<span style="font-weight:bold">=</span>np<span style="font-weight:bold">.</span>arange(<span style="color:#099">78</span>)<span style="font-weight:bold">.</span>astype(<span style="color:#b84">&#39;str&#39;</span>))<span style="font-weight:bold">.</span>ix[<span style="font-weight:bold">-</span>window<span style="font-weight:bold">*</span><span style="color:#099">20</span><span style="font-weight:bold">-</span><span style="color:#099">1</span>:,:]
</span></span></code></pre></div><p>Some further data investigation.</p>
<p>Unshifted data for training:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="font-weight:bold">for</span> f <span style="font-weight:bold">in</span> <span style="color:#999">range</span>(<span style="color:#099">17</span>,<span style="color:#099">78</span>): X_<span style="font-weight:bold">.</span>ix[:,f] <span style="font-weight:bold">=</span> X_<span style="font-weight:bold">.</span>ix[:,f]<span style="font-weight:bold">.</span>astype(<span style="color:#b84">&#39;category&#39;</span>)
</span></span><span style="display:flex;"><span>y_ <span style="font-weight:bold">=</span> (price_df<span style="font-weight:bold">.</span>ix[X_<span style="font-weight:bold">.</span>index,<span style="color:#b84">&#39;close&#39;</span>] <span style="font-weight:bold">&gt;</span> price_df<span style="font-weight:bold">.</span>ix[X_<span style="font-weight:bold">.</span>index,<span style="color:#b84">&#39;open&#39;</span>])<span style="font-weight:bold">.</span>astype(<span style="color:#b84">&#39;int&#39;</span>)
</span></span></code></pre></div><p>Shift the data so that they corresponds with</p>
<p>$$
y_i = \text{clf}(X_i).
$$</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>X <span style="font-weight:bold">=</span> X_<span style="font-weight:bold">.</span>ix[:<span style="font-weight:bold">-</span><span style="color:#099">1</span>,:]
</span></span><span style="display:flex;"><span>y <span style="font-weight:bold">=</span> y_[<span style="color:#099">1</span>:]
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(X<span style="font-weight:bold">.</span>shape)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(y<span style="font-weight:bold">.</span>shape)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(y<span style="font-weight:bold">.</span>value_counts())
</span></span></code></pre></div><pre><code>(60, 78)
(60,)
1    32
0    28
dtype: int64
</code></pre>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>y<span style="font-weight:bold">.</span>describe()
</span></span></code></pre></div><pre><code>count    60.000000
mean      0.533333
std       0.503098
min       0.000000
25%       0.000000
50%       1.000000
75%       1.000000
max       1.000000
dtype: float64
</code></pre>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>X<span style="font-weight:bold">.</span>describe(include<span style="font-weight:bold">=</span>[<span style="color:#b84">&#39;number&#39;</span>])
</span></span></code></pre></div><table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">0</th>
<th style="text-align:center">1</th>
<th style="text-align:center">2</th>
<th style="text-align:center">3</th>
<th style="text-align:center">4</th>
<th style="text-align:center">5</th>
<th style="text-align:center">6</th>
<th style="text-align:center">7</th>
<th style="text-align:center">8</th>
<th style="text-align:center">9</th>
<th style="text-align:center">10</th>
<th style="text-align:center">11</th>
<th style="text-align:center">12</th>
<th style="text-align:center">13</th>
<th style="text-align:center">14</th>
<th style="text-align:center">15</th>
<th style="text-align:center">16</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">count</td>
<td style="text-align:center">60.000000</td>
<td style="text-align:center">60.000000</td>
<td style="text-align:center">60.000000</td>
<td style="text-align:center">60.000000</td>
<td style="text-align:center">60.000000</td>
<td style="text-align:center">60.000000</td>
<td style="text-align:center">60.000000</td>
<td style="text-align:center">60.000000</td>
<td style="text-align:center">60.000000</td>
<td style="text-align:center">60.000000</td>
<td style="text-align:center">60.000000</td>
<td style="text-align:center">60.000000</td>
<td style="text-align:center">60.000000</td>
<td style="text-align:center">60.000000</td>
<td style="text-align:center">6.000000e+01</td>
<td style="text-align:center">60.000000</td>
<td style="text-align:center">60.000000</td>
</tr>
<tr>
<td style="text-align:center">mean</td>
<td style="text-align:center">0.976212</td>
<td style="text-align:center">25.990400</td>
<td style="text-align:center">25.656650</td>
<td style="text-align:center">24.940308</td>
<td style="text-align:center">22.879636</td>
<td style="text-align:center">21.834747</td>
<td style="text-align:center">20.977319</td>
<td style="text-align:center">39.063571</td>
<td style="text-align:center">35.434052</td>
<td style="text-align:center">0.852667</td>
<td style="text-align:center">67.581394</td>
<td style="text-align:center">26.807436</td>
<td style="text-align:center">25.990400</td>
<td style="text-align:center">25.173364</td>
<td style="text-align:center">1.039211e+09</td>
<td style="text-align:center">0.723869</td>
<td style="text-align:center">24.547501</td>
</tr>
<tr>
<td style="text-align:center">std</td>
<td style="text-align:center">0.015776</td>
<td style="text-align:center">2.827868</td>
<td style="text-align:center">2.770969</td>
<td style="text-align:center">2.614787</td>
<td style="text-align:center">1.668471</td>
<td style="text-align:center">1.437906</td>
<td style="text-align:center">1.220115</td>
<td style="text-align:center">7.954479</td>
<td style="text-align:center">7.286649</td>
<td style="text-align:center">0.354867</td>
<td style="text-align:center">7.811732</td>
<td style="text-align:center">3.090838</td>
<td style="text-align:center">2.827868</td>
<td style="text-align:center">2.616494</td>
<td style="text-align:center">1.263014e+08</td>
<td style="text-align:center">0.168018</td>
<td style="text-align:center">4.961348</td>
</tr>
<tr>
<td style="text-align:center">min</td>
<td style="text-align:center">0.948008</td>
<td style="text-align:center">22.640000</td>
<td style="text-align:center">22.085000</td>
<td style="text-align:center">21.392000</td>
<td style="text-align:center">20.650167</td>
<td style="text-align:center">19.762876</td>
<td style="text-align:center">19.285863</td>
<td style="text-align:center">25.863478</td>
<td style="text-align:center">26.096620</td>
<td style="text-align:center">0.413193</td>
<td style="text-align:center">54.834433</td>
<td style="text-align:center">22.887871</td>
<td style="text-align:center">22.640000</td>
<td style="text-align:center">22.083778</td>
<td style="text-align:center">8.750972e+08</td>
<td style="text-align:center">0.527858</td>
<td style="text-align:center">16.936824</td>
</tr>
<tr>
<td style="text-align:center">25%</td>
<td style="text-align:center">0.965606</td>
<td style="text-align:center">23.293000</td>
<td style="text-align:center">23.043000</td>
<td style="text-align:center">22.981875</td>
<td style="text-align:center">21.499667</td>
<td style="text-align:center">20.572173</td>
<td style="text-align:center">19.949996</td>
<td style="text-align:center">32.360567</td>
<td style="text-align:center">27.708563</td>
<td style="text-align:center">0.528046</td>
<td style="text-align:center">61.687466</td>
<td style="text-align:center">23.800034</td>
<td style="text-align:center">23.293000</td>
<td style="text-align:center">22.799033</td>
<td style="text-align:center">9.242619e+08</td>
<td style="text-align:center">0.578824</td>
<td style="text-align:center">20.770453</td>
</tr>
<tr>
<td style="text-align:center">50%</td>
<td style="text-align:center">0.974243</td>
<td style="text-align:center">25.244000</td>
<td style="text-align:center">24.591000</td>
<td style="text-align:center">24.007250</td>
<td style="text-align:center">22.387833</td>
<td style="text-align:center">21.624111</td>
<td style="text-align:center">20.712315</td>
<td style="text-align:center">39.207117</td>
<td style="text-align:center">34.467175</td>
<td style="text-align:center">0.749777</td>
<td style="text-align:center">66.749465</td>
<td style="text-align:center">26.030462</td>
<td style="text-align:center">25.244000</td>
<td style="text-align:center">24.397322</td>
<td style="text-align:center">1.038170e+09</td>
<td style="text-align:center">0.656475</td>
<td style="text-align:center">23.760415</td>
</tr>
<tr>
<td style="text-align:center">75%</td>
<td style="text-align:center">0.987142</td>
<td style="text-align:center">29.210500</td>
<td style="text-align:center">28.633250</td>
<td style="text-align:center">27.278500</td>
<td style="text-align:center">24.174250</td>
<td style="text-align:center">23.017472</td>
<td style="text-align:center">21.933203</td>
<td style="text-align:center">46.661330</td>
<td style="text-align:center">41.646060</td>
<td style="text-align:center">1.179799</td>
<td style="text-align:center">73.758827</td>
<td style="text-align:center">30.086332</td>
<td style="text-align:center">29.210500</td>
<td style="text-align:center">27.848188</td>
<td style="text-align:center">1.159543e+09</td>
<td style="text-align:center">0.852259</td>
<td style="text-align:center">27.627880</td>
</tr>
<tr>
<td style="text-align:center">max</td>
<td style="text-align:center">1.007653</td>
<td style="text-align:center">30.286000</td>
<td style="text-align:center">29.820000</td>
<td style="text-align:center">29.691500</td>
<td style="text-align:center">26.243333</td>
<td style="text-align:center">24.528111</td>
<td style="text-align:center">23.411667</td>
<td style="text-align:center">49.526365</td>
<td style="text-align:center">46.601986</td>
<td style="text-align:center">1.427553</td>
<td style="text-align:center">82.767827</td>
<td style="text-align:center">31.203224</td>
<td style="text-align:center">30.286000</td>
<td style="text-align:center">29.442553</td>
<td style="text-align:center">1.301344e+09</td>
<td style="text-align:center">1.065128</td>
<td style="text-align:center">35.042981</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>X<span style="font-weight:bold">.</span>describe(include<span style="font-weight:bold">=</span>[<span style="color:#b84">&#39;category&#39;</span>])
</span></span></code></pre></div><table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">17</th>
<th style="text-align:center">18</th>
<th style="text-align:center">19</th>
<th style="text-align:center">20</th>
<th style="text-align:center">21</th>
<th style="text-align:center">22</th>
<th style="text-align:center">23</th>
<th style="text-align:center">24</th>
<th style="text-align:center">25</th>
<th style="text-align:center">26</th>
<th style="text-align:center">27</th>
<th style="text-align:center">28</th>
<th style="text-align:center">29</th>
<th style="text-align:center">30</th>
<th style="text-align:center">31</th>
<th style="text-align:center">32</th>
<th style="text-align:center">33</th>
<th style="text-align:center">34</th>
<th style="text-align:center">35</th>
<th style="text-align:center">36</th>
<th style="text-align:center">37</th>
<th style="text-align:center">38</th>
<th style="text-align:center">39</th>
<th style="text-align:center">40</th>
<th style="text-align:center">41</th>
<th style="text-align:center">42</th>
<th style="text-align:center">43</th>
<th style="text-align:center">44</th>
<th style="text-align:center">45</th>
<th style="text-align:center">46</th>
<th style="text-align:center">47</th>
<th style="text-align:center">48</th>
<th style="text-align:center">49</th>
<th style="text-align:center">50</th>
<th style="text-align:center">51</th>
<th style="text-align:center">52</th>
<th style="text-align:center">53</th>
<th style="text-align:center">54</th>
<th style="text-align:center">55</th>
<th style="text-align:center">56</th>
<th style="text-align:center">57</th>
<th style="text-align:center">58</th>
<th style="text-align:center">59</th>
<th style="text-align:center">60</th>
<th style="text-align:center">61</th>
<th style="text-align:center">62</th>
<th style="text-align:center">63</th>
<th style="text-align:center">64</th>
<th style="text-align:center">65</th>
<th style="text-align:center">66</th>
<th style="text-align:center">67</th>
<th style="text-align:center">68</th>
<th style="text-align:center">69</th>
<th style="text-align:center">70</th>
<th style="text-align:center">71</th>
<th style="text-align:center">72</th>
<th style="text-align:center">73</th>
<th style="text-align:center">74</th>
<th style="text-align:center">75</th>
<th style="text-align:center">76</th>
<th style="text-align:center">77</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">count</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
</tr>
<tr>
<td style="text-align:center">unique</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">2.0</td>
<td style="text-align:center">2.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">2.0</td>
<td style="text-align:center">3.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">3.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">2.0</td>
<td style="text-align:center">2.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">3.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">2.0</td>
<td style="text-align:center">2.0</td>
<td style="text-align:center">2.0</td>
<td style="text-align:center">3.0</td>
<td style="text-align:center">3.0</td>
<td style="text-align:center">3.0</td>
<td style="text-align:center">4.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">2.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">2.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">2.0</td>
<td style="text-align:center">3.0</td>
<td style="text-align:center">3.0</td>
<td style="text-align:center">2.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">2.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">2.0</td>
<td style="text-align:center">2.0</td>
<td style="text-align:center">3.0</td>
<td style="text-align:center">3.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">2.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">1.0</td>
</tr>
<tr>
<td style="text-align:center">top</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
<td style="text-align:center">0.0</td>
</tr>
<tr>
<td style="text-align:center">freq</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">59.0</td>
<td style="text-align:center">57.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">59.0</td>
<td style="text-align:center">47.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">55.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">59.0</td>
<td style="text-align:center">51.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">53.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">59.0</td>
<td style="text-align:center">59.0</td>
<td style="text-align:center">59.0</td>
<td style="text-align:center">55.0</td>
<td style="text-align:center">58.0</td>
<td style="text-align:center">49.0</td>
<td style="text-align:center">51.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">59.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">59.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">51.0</td>
<td style="text-align:center">48.0</td>
<td style="text-align:center">57.0</td>
<td style="text-align:center">57.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">55.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">59.0</td>
<td style="text-align:center">59.0</td>
<td style="text-align:center">51.0</td>
<td style="text-align:center">41.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">59.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
<td style="text-align:center">60.0</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>unbalance <span style="font-weight:bold">=</span> <span style="color:#999">sum</span>(y<span style="font-weight:bold">==</span><span style="color:#099">1</span>)<span style="font-weight:bold">/</span><span style="color:#999">len</span>(y)
</span></span><span style="display:flex;"><span><span style="font-weight:bold">if</span> <span style="font-weight:bold">not</span> (<span style="color:#099">2</span><span style="font-weight:bold">/</span><span style="color:#099">5</span> <span style="font-weight:bold">&lt;</span> unbalance <span style="font-weight:bold">&lt;</span> <span style="color:#099">3</span><span style="font-weight:bold">/</span><span style="color:#099">5</span>): <span style="color:#999">print</span>(<span style="color:#b84">&#39;Unbalanced dataset!!!&#39;</span>)
</span></span><span style="display:flex;"><span>unbalance
</span></span></code></pre></div><pre><code>0.53333333333333333
</code></pre>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>data <span style="font-weight:bold">=</span> X
</span></span><span style="display:flex;"><span>data<span style="font-weight:bold">.</span>loc[:,<span style="color:#b84">&#39;y&#39;</span>] <span style="font-weight:bold">=</span> y<span style="font-weight:bold">.</span>values
</span></span><span style="display:flex;"><span>data<span style="font-weight:bold">.</span>columns
</span></span></code></pre></div><pre><code>Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',
       '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24',
       '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36',
       '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48',
       '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60',
       '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72',
       '73', '74', '75', '76', '77', 'y'],
      dtype='object')
</code></pre>
<p>Quick look at the distribution of y.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="font-weight:bold">if</span> verbose:
</span></span><span style="display:flex;"><span>    ax <span style="font-weight:bold">=</span> plt<span style="font-weight:bold">.</span>axes()
</span></span><span style="display:flex;"><span>    sns<span style="font-weight:bold">.</span>countplot(x<span style="font-weight:bold">=</span><span style="color:#b84">&#39;y&#39;</span>, data<span style="font-weight:bold">=</span>data, ax<span style="font-weight:bold">=</span>ax)
</span></span><span style="display:flex;"><span>    ax<span style="font-weight:bold">.</span>set_title(<span style="color:#b84">&#39;Actual y Distribution (total: </span><span style="color:#b84">{}</span><span style="color:#b84">)&#39;</span><span style="font-weight:bold">.</span>format(<span style="color:#999">len</span>(data)))
</span></span><span style="display:flex;"><span>    plt<span style="font-weight:bold">.</span>show()
</span></span></code></pre></div><img src="/images/output_20_0.png" style="max-width: min(500px, 100%);"/>
<p>At first we can see that the target variable is distributed quite equally. We won&rsquo;t perform any actions to deal with imbalanced dataset. First we present the continuous data using boxplot (described in the following image)</p>
<img src="https://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/images/schematic.png" style="max-width: min(600px, 100%);"/>
<p>Boxplot of y against continuous variables.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="font-weight:bold">if</span> verbose:
</span></span><span style="display:flex;"><span>    f, axarr <span style="font-weight:bold">=</span> plt<span style="font-weight:bold">.</span>subplots(<span style="color:#099">4</span>, <span style="color:#099">4</span>, figsize<span style="font-weight:bold">=</span>(<span style="color:#099">15</span>, <span style="color:#099">15</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="font-weight:bold">for</span> i <span style="font-weight:bold">in</span> <span style="color:#999">range</span>(<span style="color:#099">16</span>):
</span></span><span style="display:flex;"><span>        sns<span style="font-weight:bold">.</span>boxplot(x<span style="font-weight:bold">=</span><span style="color:#999">str</span>(i), y<span style="font-weight:bold">=</span><span style="color:#b84">&#39;y&#39;</span>, data<span style="font-weight:bold">=</span>data, showmeans<span style="font-weight:bold">=</span><span style="font-weight:bold">True</span>, ax<span style="font-weight:bold">=</span>axarr[i<span style="font-weight:bold">//</span><span style="color:#099">4</span>,i<span style="font-weight:bold">%</span><span style="color:#099">4</span>])
</span></span><span style="display:flex;"><span>        axarr[i<span style="font-weight:bold">//</span><span style="color:#099">4</span>,i<span style="font-weight:bold">%</span><span style="color:#099">4</span>]<span style="font-weight:bold">.</span>set_title(<span style="color:#b84">&#39;factor &#39;</span><span style="font-weight:bold">+</span><span style="color:#999">str</span>(i))
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    plt<span style="font-weight:bold">.</span>tight_layout()
</span></span><span style="display:flex;"><span>    plt<span style="font-weight:bold">.</span>show()
</span></span></code></pre></div><p><img src="/images/output_23_0.png" alt=""></p>
<p>Pairplot of all continous variables.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="font-weight:bold">if</span> verbose:
</span></span><span style="display:flex;"><span>    sns<span style="font-weight:bold">.</span>pairplot(data, <span style="color:#999">vars</span><span style="font-weight:bold">=</span>[<span style="color:#999">str</span>(i) <span style="font-weight:bold">for</span> i <span style="font-weight:bold">in</span> <span style="color:#999">range</span>(<span style="color:#099">1</span>,<span style="color:#099">17</span>)], hue<span style="font-weight:bold">=</span><span style="color:#b84">&#39;y&#39;</span>, size<span style="font-weight:bold">=</span><span style="color:#099">3</span>)
</span></span><span style="display:flex;"><span>    plt<span style="font-weight:bold">.</span>show()
</span></span></code></pre></div><p><img src="/images/output_25_0.png" alt=""></p>
<p>Dummy encoding for categorical variables.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="font-weight:bold">for</span> i <span style="font-weight:bold">in</span> <span style="color:#999">range</span>(<span style="color:#099">17</span>,<span style="color:#099">78</span>):
</span></span><span style="display:flex;"><span>    dummies <span style="font-weight:bold">=</span> pd<span style="font-weight:bold">.</span>get_dummies(data<span style="font-weight:bold">.</span>ix[:,i])
</span></span><span style="display:flex;"><span>    dummies <span style="font-weight:bold">=</span> dummies<span style="font-weight:bold">.</span>add_prefix(<span style="color:#b84">&#34;</span><span style="color:#b84">{}</span><span style="color:#b84">#&#34;</span><span style="font-weight:bold">.</span>format(i))
</span></span><span style="display:flex;"><span>    data<span style="font-weight:bold">.</span>drop(<span style="color:#999">str</span>(i), axis<span style="font-weight:bold">=</span><span style="color:#099">1</span>, inplace<span style="font-weight:bold">=</span><span style="font-weight:bold">True</span>)
</span></span><span style="display:flex;"><span>    data <span style="font-weight:bold">=</span> data<span style="font-weight:bold">.</span>join(dummies)
</span></span></code></pre></div><h1 id="section-3-feature-selection">Section 3: Feature Selection</h1>
<p>Drop columns that contains only one value.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>mask <span style="font-weight:bold">=</span> data<span style="font-weight:bold">.</span>std() <span style="font-weight:bold">==</span> <span style="color:#099">0</span>
</span></span><span style="display:flex;"><span>data_valid <span style="font-weight:bold">=</span> data<span style="font-weight:bold">.</span>ix[:,<span style="font-weight:bold">~</span>mask]
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>X <span style="font-weight:bold">=</span> data_valid<span style="font-weight:bold">.</span>drop(<span style="color:#b84">&#39;y&#39;</span>, axis<span style="font-weight:bold">=</span><span style="color:#099">1</span>)
</span></span><span style="display:flex;"><span>y <span style="font-weight:bold">=</span> data_valid[<span style="color:#b84">&#39;y&#39;</span>]
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(X<span style="font-weight:bold">.</span>columns<span style="font-weight:bold">.</span>tolist())
</span></span></code></pre></div><pre><code>['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '19#0.0', '19#100.0', '21#-100.0', '21#0.0', '24#-100.0', '24#0.0', '26#-100.0', '26#0.0', '26#100.0', '28#0.0', '28#100.0', '29#-100.0', '29#0.0', '30#-100.0', '30#0.0', '30#100.0', '31#-200.0', '31#-100.0', '31#0.0', '31#100.0', '32#0.0', '32#100.0', '36#-100.0', '36#0.0', '36#100.0', '37#0.0', '37#100.0', '40#0.0', '40#100.0', '41#0.0', '41#100.0', '42#-100.0', '42#0.0', '42#100.0', '45#-100.0', '45#0.0', '49#0', '49#1', '51#0', '51#1', '53#0', '53#1', '54#0', '54#1', '55#0', '55#1', '56#0', '56#1', '57#0', '57#1', '58#0', '58#1', '59#0', '59#1', '60#0', '60#1', '61#0', '61#1', '62#0', '62#1', '64#0', '64#1', '65#0', '65#1', '66#0', '66#1', '68#0', '68#1', '69#0', '69#1', '70#0', '70#1', '72#0', '72#1', '75#0', '75#1', '76#0', '76#1']
</code></pre>
<p><code>Variance Ranking</code></p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>vt <span style="font-weight:bold">=</span> VarianceThreshold()<span style="font-weight:bold">.</span>fit(X)
</span></span><span style="display:flex;"><span>ranks <span style="font-weight:bold">=</span> (<span style="font-weight:bold">-</span>vt<span style="font-weight:bold">.</span>variances_)<span style="font-weight:bold">.</span>argsort()<span style="font-weight:bold">.</span>argsort()
</span></span><span style="display:flex;"><span>feat_var_threshold <span style="font-weight:bold">=</span> X<span style="font-weight:bold">.</span>columns[ranks<span style="font-weight:bold">&lt;=</span><span style="color:#099">20</span>]
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(feat_var_threshold<span style="font-weight:bold">.</span>tolist())
</span></span></code></pre></div><pre><code>['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '16', '31#0.0', '36#0.0', '42#0.0', '65#0', '70#0', '70#1']
</code></pre>
<p><code>Random Forest</code></p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>model <span style="font-weight:bold">=</span> RandomForestClassifier()
</span></span><span style="display:flex;"><span>model<span style="font-weight:bold">.</span>fit(X,y)
</span></span><span style="display:flex;"><span>feature_imp <span style="font-weight:bold">=</span> pd<span style="font-weight:bold">.</span>DataFrame(model<span style="font-weight:bold">.</span>feature_importances_, index<span style="font-weight:bold">=</span>X<span style="font-weight:bold">.</span>columns, columns<span style="font-weight:bold">=</span>[<span style="color:#b84">&#39;importance&#39;</span>])
</span></span><span style="display:flex;"><span>feat_imp_20 <span style="font-weight:bold">=</span> feature_imp<span style="font-weight:bold">.</span>sort_values(<span style="color:#b84">&#39;importance&#39;</span>, ascending<span style="font-weight:bold">=</span><span style="font-weight:bold">False</span>)<span style="font-weight:bold">.</span>head(<span style="color:#099">20</span>)<span style="font-weight:bold">.</span>index
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(feat_imp_20<span style="font-weight:bold">.</span>tolist())
</span></span></code></pre></div><pre><code>['16', '2', '13', '14', '9', '8', '0', '1', '15', '10', '7', '6', '68#0', '70#1', '4', '64#0', '11', '30#0.0', '3', '5']
</code></pre>
<p><code>Chi2 Test</code></p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>X_minmax <span style="font-weight:bold">=</span> MinMaxScaler([<span style="color:#099">0</span>,<span style="color:#099">1</span>])<span style="font-weight:bold">.</span>fit_transform(X)
</span></span><span style="display:flex;"><span>X_scored <span style="font-weight:bold">=</span> SelectKBest(score_func<span style="font-weight:bold">=</span>chi2, k<span style="font-weight:bold">=</span><span style="color:#b84">&#39;all&#39;</span>)<span style="font-weight:bold">.</span>fit(X_minmax, y)
</span></span><span style="display:flex;"><span>feature_scoring <span style="font-weight:bold">=</span> pd<span style="font-weight:bold">.</span>DataFrame({<span style="color:#b84">&#39;feature&#39;</span>: X<span style="font-weight:bold">.</span>columns,<span style="color:#b84">&#39;score&#39;</span>: X_scored<span style="font-weight:bold">.</span>scores_})
</span></span><span style="display:flex;"><span>feat_scored_20 <span style="font-weight:bold">=</span> feature_scoring<span style="font-weight:bold">.</span>sort_values(<span style="color:#b84">&#39;score&#39;</span>, ascending<span style="font-weight:bold">=</span><span style="font-weight:bold">False</span>)<span style="font-weight:bold">.</span>head(<span style="color:#099">20</span>)[<span style="color:#b84">&#39;feature&#39;</span>]<span style="font-weight:bold">.</span>values
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(feat_scored_20<span style="font-weight:bold">.</span>tolist())
</span></span></code></pre></div><pre><code>['42#-100.0', '64#1', '36#-100.0', '70#0', '68#1', '40#100.0', '31#-100.0', '60#1', '76#1', '51#0', '30#100.0', '32#100.0', '24#-100.0', '21#-100.0', '53#0', '30#-100.0', '57#1', '58#1', '59#1', '62#1']
</code></pre>
<p><code>Recursive Feature Elimination (RFE)</code></p>
<p>with logistic regression model.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>rfe <span style="font-weight:bold">=</span> RFE(LogisticRegression(),<span style="color:#099">20</span>)
</span></span><span style="display:flex;"><span>rfe<span style="font-weight:bold">.</span>fit(X, y)
</span></span><span style="display:flex;"><span>feature_rfe_scoring <span style="font-weight:bold">=</span> pd<span style="font-weight:bold">.</span>DataFrame({<span style="color:#b84">&#39;feature&#39;</span>: X<span style="font-weight:bold">.</span>columns,<span style="color:#b84">&#39;score&#39;</span>: rfe<span style="font-weight:bold">.</span>ranking_})
</span></span><span style="display:flex;"><span>feat_rfe_20 <span style="font-weight:bold">=</span> feature_rfe_scoring[feature_rfe_scoring[<span style="color:#b84">&#39;score&#39;</span>] <span style="font-weight:bold">==</span> <span style="color:#099">1</span>][<span style="color:#b84">&#39;feature&#39;</span>]<span style="font-weight:bold">.</span>values
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(feat_rfe_20<span style="font-weight:bold">.</span>tolist())
</span></span></code></pre></div><pre><code>['1', '2', '3', '4', '5', '6', '7', '8', '10', '11', '12', '13', '14', '16', '36#0.0', '40#0.0', '42#0.0', '68#0', '70#0', '70#1']
</code></pre>
<p><code>Final selection of features</code></p>
<p>is the union of all previous sets.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>features <span style="font-weight:bold">=</span> np<span style="font-weight:bold">.</span>hstack([feat_var_threshold,feat_imp_20,feat_scored_20,feat_rfe_20])
</span></span><span style="display:flex;"><span>features <span style="font-weight:bold">=</span> np<span style="font-weight:bold">.</span>unique(features)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(<span style="color:#b84">&#39;Final features (</span><span style="color:#b84">{}</span><span style="color:#b84"> in total): </span><span style="color:#b84">{}</span><span style="color:#b84">&#39;</span><span style="font-weight:bold">.</span>format(<span style="color:#999">len</span>(features),<span style="color:#b84">&#39;, &#39;</span><span style="font-weight:bold">.</span>join(features)))
</span></span></code></pre></div><pre><code>Final features (46 in total): 0, 1, 10, 11, 12, 13, 14, 15, 16, 2, 21#-100.0, 24#-100.0, 3, 30#-100.0, 30#0.0, 30#100.0, 31#-100.0, 31#0.0, 32#100.0, 36#-100.0, 36#0.0, 4, 40#0.0, 40#100.0, 42#-100.0, 42#0.0, 5, 51#0, 53#0, 57#1, 58#1, 59#1, 6, 60#1, 62#1, 64#0, 64#1, 65#0, 68#0, 68#1, 7, 70#0, 70#1, 76#1, 8, 9
</code></pre>
<h1 id="section-4-model-training">Section 4: Model Training</h1>
<p><strong>1. Split the training and testing data (ratio: 3:1).</strong></p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>data_clean <span style="font-weight:bold">=</span> data_valid<span style="font-weight:bold">.</span>ix[:,features<span style="font-weight:bold">.</span>tolist()<span style="font-weight:bold">+</span>[<span style="color:#b84">&#39;y&#39;</span>]]
</span></span><span style="display:flex;"><span>tot_len <span style="font-weight:bold">=</span> <span style="color:#999">len</span>(data_clean)
</span></span><span style="display:flex;"><span>split_len <span style="font-weight:bold">=</span> tot_len <span style="font-weight:bold">*</span> <span style="color:#099">3</span> <span style="font-weight:bold">//</span> <span style="color:#099">4</span>
</span></span><span style="display:flex;"><span>X_train <span style="font-weight:bold">=</span> data_clean<span style="font-weight:bold">.</span>ix[:split_len,features]
</span></span><span style="display:flex;"><span>X_test <span style="font-weight:bold">=</span> data_clean<span style="font-weight:bold">.</span>ix[split_len:,features]
</span></span><span style="display:flex;"><span>y_train <span style="font-weight:bold">=</span> data_clean<span style="font-weight:bold">.</span>ix[:split_len,<span style="color:#b84">&#39;y&#39;</span>]
</span></span><span style="display:flex;"><span>y_test <span style="font-weight:bold">=</span> data_clean<span style="font-weight:bold">.</span>ix[split_len:,<span style="color:#b84">&#39;y&#39;</span>]
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(<span style="color:#b84">&#39;Clean dataset shape: </span><span style="color:#b84">{}</span><span style="color:#b84">&#39;</span><span style="font-weight:bold">.</span>format(data_clean<span style="font-weight:bold">.</span>shape))
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(<span style="color:#b84">&#39;Train features shape: </span><span style="color:#b84">{}</span><span style="color:#b84">&#39;</span><span style="font-weight:bold">.</span>format(X_train<span style="font-weight:bold">.</span>shape))
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(<span style="color:#b84">&#39;Test features shape: </span><span style="color:#b84">{}</span><span style="color:#b84">&#39;</span><span style="font-weight:bold">.</span>format(X_test<span style="font-weight:bold">.</span>shape))
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(<span style="color:#b84">&#39;Train label shape: </span><span style="color:#b84">{}</span><span style="color:#b84">&#39;</span><span style="font-weight:bold">.</span> <span style="color:#999">format</span>(y_train<span style="font-weight:bold">.</span>shape))
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(<span style="color:#b84">&#39;Test label shape: </span><span style="color:#b84">{}</span><span style="color:#b84">&#39;</span><span style="font-weight:bold">.</span> <span style="color:#999">format</span>(y_test<span style="font-weight:bold">.</span>shape))
</span></span></code></pre></div><pre><code>Clean dataset shape: (60, 47)
Train features shape: (45, 46)
Test features shape: (15, 46)
Train label shape: (45,)
Test label shape: (15,)
</code></pre>
<p><strong>2. PCA visualization of training data</strong></p>
<p>PCA plot</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="font-weight:bold">if</span> verbose:
</span></span><span style="display:flex;"><span>    components <span style="font-weight:bold">=</span> <span style="color:#099">8</span>
</span></span><span style="display:flex;"><span>    pca <span style="font-weight:bold">=</span> PCA(n_components<span style="font-weight:bold">=</span>components)<span style="font-weight:bold">.</span>fit(X_train)
</span></span><span style="display:flex;"><span>    pca_variance_explained_df <span style="font-weight:bold">=</span> pd<span style="font-weight:bold">.</span>DataFrame({<span style="color:#b84">&#39;component&#39;</span>: np<span style="font-weight:bold">.</span>arange(<span style="color:#099">1</span>, components<span style="font-weight:bold">+</span><span style="color:#099">1</span>),<span style="color:#b84">&#39;variance_explained&#39;</span>: pca<span style="font-weight:bold">.</span>explained_variance_ratio_})
</span></span><span style="display:flex;"><span>    ax <span style="font-weight:bold">=</span> sns<span style="font-weight:bold">.</span>barplot(x<span style="font-weight:bold">=</span><span style="color:#b84">&#39;component&#39;</span>, y<span style="font-weight:bold">=</span><span style="color:#b84">&#39;variance_explained&#39;</span>, data<span style="font-weight:bold">=</span>pca_variance_explained_df)
</span></span><span style="display:flex;"><span>    ax<span style="font-weight:bold">.</span>set_title(<span style="color:#b84">&#39;PCA - Variance explained&#39;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="font-weight:bold">.</span>show()
</span></span></code></pre></div><img src="/images/output_46_0.png" style="max-width: min(500px, 100%);"/>
<p>Lmplot</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="font-weight:bold">if</span> verbose:
</span></span><span style="display:flex;"><span>    X_pca <span style="font-weight:bold">=</span> pd<span style="font-weight:bold">.</span>DataFrame(pca<span style="font-weight:bold">.</span>transform(X_train)[:,:<span style="color:#099">2</span>])
</span></span><span style="display:flex;"><span>    X_pca[<span style="color:#b84">&#39;target&#39;</span>] <span style="font-weight:bold">=</span> y_train<span style="font-weight:bold">.</span>values
</span></span><span style="display:flex;"><span>    X_pca<span style="font-weight:bold">.</span>columns <span style="font-weight:bold">=</span> [<span style="color:#b84">&#34;x&#34;</span>, <span style="color:#b84">&#34;y&#34;</span>, <span style="color:#b84">&#34;target&#34;</span>]
</span></span><span style="display:flex;"><span>    sns<span style="font-weight:bold">.</span>lmplot(<span style="color:#b84">&#39;x&#39;</span>,<span style="color:#b84">&#39;y&#39;</span>, 
</span></span><span style="display:flex;"><span>               data<span style="font-weight:bold">=</span>X_pca, 
</span></span><span style="display:flex;"><span>               hue<span style="font-weight:bold">=</span><span style="color:#b84">&#34;target&#34;</span>, 
</span></span><span style="display:flex;"><span>               fit_reg<span style="font-weight:bold">=</span><span style="font-weight:bold">False</span>, 
</span></span><span style="display:flex;"><span>               markers<span style="font-weight:bold">=</span>[<span style="color:#b84">&#34;o&#34;</span>, <span style="color:#b84">&#34;x&#34;</span>], 
</span></span><span style="display:flex;"><span>               palette<span style="font-weight:bold">=</span><span style="color:#b84">&#34;Set1&#34;</span>, 
</span></span><span style="display:flex;"><span>               size<span style="font-weight:bold">=</span><span style="color:#099">7</span>,
</span></span><span style="display:flex;"><span>               scatter_kws<span style="font-weight:bold">=</span>{<span style="color:#b84">&#34;alpha&#34;</span>: <span style="color:#099">.2</span>})
</span></span><span style="display:flex;"><span>    plt<span style="font-weight:bold">.</span>show()
</span></span></code></pre></div><img src="/images/output_48_0.png" style="max-width: min(600px, 100%);"/>
<p><strong>3. A New Accuracy Metric Based on Utility and Risk-Aversion</strong></p>
<p>Instead of using existing accuracy or error metrics, e.g. accuracy scores and log loss, we tend to come up with our own metric that suits this scenario better. According to classical utility theory, the utility on the expected net return of a transaction should at least follow these properties:</p>
<ol>
<li>Higher expected returns means higher utility;</li>
<li>Zero expected return means zero absolute utility;</li>
<li>Marginal utility decreases with higher expected returns;</li>
<li>The utility is robust of scaling.</li>
</ol>
<p>Mathematically, therefore, we know a well-behaved utility function <code>$U(x)$</code> has:</p>
<ol>
<li>A non-negative first-order derivative;</li>
<li>A non-positive second-order derivative;</li>
<li>A zero at <code>$x=0$</code>.</li>
</ol>
<p>However, since late 20th century, this setting of utility has been widely criticized and the voice was mainly from behavioral economics. This group of people managed a huge number of empirical experiments and showed how poor such utility models work when the variation of risk-aversion is considered. Risk-aversion was originally introduced to catch the aversion of a human against uncertainty. In classical economics, there are a range of measures to depict such aversion. One of the most famous is ArrowPratt measure of absolute risk-aversion (ARA), which is defined based on the utility function:</p>
<p>$$
ARA = -\frac{U^{\prime}(x)}{U^{\prime\prime}(x)}.
$$</p>
<p>The Arrow-Pratt absolute risk-aversion is well successful not only because it catches the concavity of the utility, but also it can be expanded into many special cases, mainly w.r.t. different classical utility functions like exponential or hyperbolic absolute utility. However, it is not in line with common sense, pointed by Daniel Kahneman and Amos Tversky in their prospect theory in 1972. The theory has been well further developed since 1992 and is now accepted as a more realistic model for uncertainty perception psychologically.</p>
<p>Different from the classical expected utility theory, the prospect theory specifies the utility in the following four implications:</p>
<ol>
<li>Certainty effect: most people are risk-averse about gaining;</li>
<li>Reflection effect: most people are risk-loving about losing;</li>
<li>Loss aversion: most people are more sensitive to losses than to gains;</li>
<li>Reference dependence: most people&rsquo;s perception of uncertainty is based on the reference point.</li>
</ol>
<p>Considering here the notion of &ldquo;most people&rdquo; is typically based on the fact tha most investors are more or less risk-averse, we simplify the model by giving assumptions as follows:</p>
<ol>
<li>The first-order derivative of the utility is non-negative for all outcome;</li>
<li>For risk-averse investors, the second-order derivative of the utility is non-negative for losses while non-positive for gains;</li>
<li>For risk-loving investors, the second-order derivative of the utility is non-positive for losses while non-negative for gains;</li>
<li>For risk-neutral investors, the second-order derivative is zero everywhere</li>
</ol>
<p>while for the loss aversion implication, we don&rsquo;t take it into consideration as the influence turned out to be minuscule compared with the loss of model simplicity.</p>
<p>Therefore, with the previous four assumptions, we can easily come up with a nice utility function w.r.t. prediction accuracy:</p>
<p>$$
U(x) = sgn(x-1/2)|2x-1|^{2^{logit\left(\frac{r+1}{2}\right)}}
$$</p>
<p>where <code>$sgn(\cdot)$</code> is the sign function and <code>$logit(\cdot)$</code> is the logit function, which is also known as the inverse of the sigmoidal &ldquo;logistic&rdquo; function:</p>
<p>$$
logit(x)=\ln\left(\frac{x}{1-x}\right).
$$</p>
<p>It is easy to validate that our utility follows the configuration and because of its monotonicity and continuity in <code>$[0,1]$</code>, is a well-behaved accuracy metric for further learning algorithms.</p>
<p><code>Utility Curve</code></p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>f <span style="font-weight:bold">=</span> <span style="font-weight:bold">lambda</span> x, r: (<span style="color:#099">2</span><span style="font-weight:bold">*</span>(x<span style="font-weight:bold">&gt;</span><span style="color:#099">0.5</span>)<span style="font-weight:bold">-</span><span style="color:#099">1</span>)<span style="font-weight:bold">*</span><span style="color:#999">abs</span>(<span style="color:#099">2</span><span style="font-weight:bold">*</span>x<span style="font-weight:bold">-</span><span style="color:#099">1</span>)<span style="font-weight:bold">**</span>(<span style="color:#099">2</span><span style="font-weight:bold">**</span>logit((r<span style="font-weight:bold">+</span><span style="color:#099">1</span>)<span style="font-weight:bold">/</span><span style="color:#099">2</span>))
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="font-weight:bold">if</span> verbose:
</span></span><span style="display:flex;"><span>    colors <span style="font-weight:bold">=</span> [<span style="color:#b84">&#39;red&#39;</span>,<span style="color:#b84">&#39;orange&#39;</span>,<span style="color:#b84">&#39;yellow&#39;</span>,<span style="color:#b84">&#39;lime&#39;</span>,<span style="color:#b84">&#39;cyan&#39;</span>]
</span></span><span style="display:flex;"><span>    r_grid <span style="font-weight:bold">=</span> [<span style="font-weight:bold">-</span><span style="color:#099">0.9</span>,<span style="font-weight:bold">-</span><span style="color:#099">0.5</span>,<span style="color:#099">0</span>,<span style="color:#099">0.5</span>,<span style="color:#099">0.9</span>]
</span></span><span style="display:flex;"><span>    x_grid <span style="font-weight:bold">=</span> np<span style="font-weight:bold">.</span>linspace(<span style="color:#099">0</span>,<span style="color:#099">1</span>,<span style="color:#099">100</span>)
</span></span><span style="display:flex;"><span>    fig, ax <span style="font-weight:bold">=</span> plt<span style="font-weight:bold">.</span>subplots(figsize<span style="font-weight:bold">=</span>(<span style="color:#099">5</span>,<span style="color:#099">5</span>))
</span></span><span style="display:flex;"><span>    <span style="font-weight:bold">for</span> i <span style="font-weight:bold">in</span> <span style="color:#999">range</span>(<span style="color:#999">len</span>(r_grid)):
</span></span><span style="display:flex;"><span>        f_grid <span style="font-weight:bold">=</span> [f(x,r_grid[i]) <span style="font-weight:bold">for</span> x <span style="font-weight:bold">in</span> x_grid]
</span></span><span style="display:flex;"><span>        ax<span style="font-weight:bold">.</span>plot(x_grid, f_grid,c<span style="font-weight:bold">=</span>colors[i],label<span style="font-weight:bold">=</span><span style="color:#b84">&#39;r = </span><span style="color:#b84">{:&gt; 3.1f}</span><span style="color:#b84">&#39;</span><span style="font-weight:bold">.</span>format(r_grid[i]))
</span></span><span style="display:flex;"><span>    ax<span style="font-weight:bold">.</span>plot((<span style="color:#099">0</span>,<span style="color:#099">1</span>),(<span style="color:#099">0</span>,<span style="color:#099">0</span>),c<span style="font-weight:bold">=</span><span style="color:#b84">&#39;black&#39;</span>)
</span></span><span style="display:flex;"><span>    box <span style="font-weight:bold">=</span> ax<span style="font-weight:bold">.</span>get_position()
</span></span><span style="display:flex;"><span>    ax<span style="font-weight:bold">.</span>set_position([box<span style="font-weight:bold">.</span>x0, box<span style="font-weight:bold">.</span>y0, box<span style="font-weight:bold">.</span>width <span style="font-weight:bold">*</span> <span style="color:#099">0.8</span>, box<span style="font-weight:bold">.</span>height])
</span></span><span style="display:flex;"><span>    ax<span style="font-weight:bold">.</span>legend(loc<span style="font-weight:bold">=</span><span style="color:#b84">&#39;center left&#39;</span>, bbox_to_anchor<span style="font-weight:bold">=</span>(<span style="color:#099">1</span>, <span style="color:#099">0.5</span>))
</span></span><span style="display:flex;"><span>    ax<span style="font-weight:bold">.</span>set_ylim([<span style="font-weight:bold">-</span><span style="color:#099">1</span>,<span style="color:#099">1</span>])
</span></span><span style="display:flex;"><span>    ax<span style="font-weight:bold">.</span>set_xlim([<span style="color:#099">0</span>,<span style="color:#099">1</span>])
</span></span><span style="display:flex;"><span>    plt<span style="font-weight:bold">.</span>xlabel(<span style="color:#b84">&#39;Accuracy&#39;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="font-weight:bold">.</span>ylabel(<span style="color:#b84">&#39;Utility&#39;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="font-weight:bold">.</span>title(<span style="color:#b84">&#39;Utility Curve for Different Risk Preferences&#39;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="font-weight:bold">.</span>show()
</span></span></code></pre></div><img src="/images/output_52_0.png" style="width: 60%;"/>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="font-weight:bold">def</span> <span style="color:#900;font-weight:bold">custom_score</span>(y_true, y_pred):
</span></span><span style="display:flex;"><span>    x <span style="font-weight:bold">=</span> <span style="color:#999">sum</span>(y_pred <span style="font-weight:bold">==</span> y_true) <span style="font-weight:bold">/</span> <span style="color:#999">len</span>(y_true)
</span></span><span style="display:flex;"><span>    r <span style="font-weight:bold">=</span> risk_preference
</span></span><span style="display:flex;"><span>    <span style="font-weight:bold">return</span> f(x,r)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>seed <span style="font-weight:bold">=</span> <span style="color:#099">7</span>
</span></span><span style="display:flex;"><span>processors <span style="font-weight:bold">=</span> <span style="color:#099">1</span>
</span></span><span style="display:flex;"><span>num_folds <span style="font-weight:bold">=</span> <span style="color:#099">4</span>
</span></span><span style="display:flex;"><span>num_instances <span style="font-weight:bold">=</span> <span style="color:#999">len</span>(X_train)
</span></span><span style="display:flex;"><span>scoring <span style="font-weight:bold">=</span> make_scorer(custom_score)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>kfold <span style="font-weight:bold">=</span> KFold(n<span style="font-weight:bold">=</span>num_instances, n_folds<span style="font-weight:bold">=</span>num_folds, random_state<span style="font-weight:bold">=</span>seed)
</span></span></code></pre></div><p>First let&rsquo;s have a quick spot-check.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Some basic models</span>
</span></span><span style="display:flex;"><span>models <span style="font-weight:bold">=</span> []
</span></span><span style="display:flex;"><span>models<span style="font-weight:bold">.</span>append((<span style="color:#b84">&#39;LR&#39;</span>, LogisticRegression()))
</span></span><span style="display:flex;"><span>models<span style="font-weight:bold">.</span>append((<span style="color:#b84">&#39;LDA&#39;</span>, LinearDiscriminantAnalysis()))
</span></span><span style="display:flex;"><span>models<span style="font-weight:bold">.</span>append((<span style="color:#b84">&#39;KNN&#39;</span>, KNeighborsClassifier(n_neighbors<span style="font-weight:bold">=</span><span style="color:#099">5</span>)))
</span></span><span style="display:flex;"><span>models<span style="font-weight:bold">.</span>append((<span style="color:#b84">&#39;DT&#39;</span>, DecisionTreeClassifier()))
</span></span><span style="display:flex;"><span>models<span style="font-weight:bold">.</span>append((<span style="color:#b84">&#39;GNB&#39;</span>, GaussianNB()))
</span></span><span style="display:flex;"><span>models<span style="font-weight:bold">.</span>append((<span style="color:#b84">&#39;SVC&#39;</span>, SVC(probability<span style="font-weight:bold">=</span><span style="font-weight:bold">True</span>, class_weight<span style="font-weight:bold">=</span><span style="color:#b84">&#39;balanced&#39;</span>)))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Evaluate each model in turn</span>
</span></span><span style="display:flex;"><span>results <span style="font-weight:bold">=</span> []
</span></span><span style="display:flex;"><span>names <span style="font-weight:bold">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="font-weight:bold">for</span> name, model <span style="font-weight:bold">in</span> models:
</span></span><span style="display:flex;"><span>    cv_results <span style="font-weight:bold">=</span> cross_val_score(model, X_train, y_train, cv<span style="font-weight:bold">=</span>kfold, scoring<span style="font-weight:bold">=</span>scoring, n_jobs<span style="font-weight:bold">=</span>processors)
</span></span><span style="display:flex;"><span>    results<span style="font-weight:bold">.</span>append(cv_results)
</span></span><span style="display:flex;"><span>    names<span style="font-weight:bold">.</span>append(name)
</span></span><span style="display:flex;"><span>    <span style="color:#999">print</span>(<span style="color:#b84">&#34;</span><span style="color:#b84">{}</span><span style="color:#b84">: (</span><span style="color:#b84">{:.3f}</span><span style="color:#b84">) +/- (</span><span style="color:#b84">{:.3f}</span><span style="color:#b84">)&#34;</span><span style="font-weight:bold">.</span>format(name, cv_results<span style="font-weight:bold">.</span>mean(), cv_results<span style="font-weight:bold">.</span>std()))
</span></span></code></pre></div><pre><code>LR: (0.117) +/- (0.436)
LDA: (0.518) +/- (0.163)
KNN: (0.255) +/- (0.263)
DT: (0.255) +/- (0.263)
GNB: (-0.106) +/- (0.393)
SVC: (0.117) +/- (0.436)
</code></pre>
<p>Let&rsquo;s first look at ensemble results.</p>
<h1 id="section-5-ensemble-modeling-and-validation">Section 5: Ensemble Modeling and Validation</h1>
<p><strong>1. Bagging (Bootstrap Aggregation)</strong></p>
<p>Prediction of a bagging model is the average of all sub-models.</p>
<p><code>Bagged Decision Trees</code>
Bagged Desicion Trees performs the best when the variance is large in the dataset.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>cart <span style="font-weight:bold">=</span> DecisionTreeClassifier()
</span></span><span style="display:flex;"><span>num_trees <span style="font-weight:bold">=</span> <span style="color:#099">100</span>
</span></span><span style="display:flex;"><span>model <span style="font-weight:bold">=</span> BaggingClassifier(base_estimator<span style="font-weight:bold">=</span>cart, n_estimators<span style="font-weight:bold">=</span>num_trees, random_state<span style="font-weight:bold">=</span>seed)
</span></span><span style="display:flex;"><span>results <span style="font-weight:bold">=</span> cross_val_score(model, X_train, y_train, cv<span style="font-weight:bold">=</span>kfold, scoring<span style="font-weight:bold">=</span>scoring, n_jobs<span style="font-weight:bold">=</span>processors)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(<span style="color:#b84">&#34;(</span><span style="color:#b84">{:.3f}</span><span style="color:#b84">) +/- (</span><span style="color:#b84">{:.3f}</span><span style="color:#b84">)&#34;</span><span style="font-weight:bold">.</span>format(results<span style="font-weight:bold">.</span>mean(), results<span style="font-weight:bold">.</span>std()))
</span></span></code></pre></div><pre><code>(0.158) +/- (0.312)
</code></pre>
<p><code>Random Forest</code>
Random forest is a famous extension to bagged decision trees. It is usually more precise but slower, especially for large number of leaves.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>num_trees <span style="font-weight:bold">=</span> <span style="color:#099">100</span>
</span></span><span style="display:flex;"><span>num_features <span style="font-weight:bold">=</span> <span style="color:#099">10</span>
</span></span><span style="display:flex;"><span>model <span style="font-weight:bold">=</span> RandomForestClassifier(n_estimators<span style="font-weight:bold">=</span>num_trees, max_features<span style="font-weight:bold">=</span>num_features)
</span></span><span style="display:flex;"><span>results <span style="font-weight:bold">=</span> cross_val_score(model, X_train, y_train, cv<span style="font-weight:bold">=</span>kfold, scoring<span style="font-weight:bold">=</span>scoring, n_jobs<span style="font-weight:bold">=</span>processors)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(<span style="color:#b84">&#34;(</span><span style="color:#b84">{:.3f}</span><span style="color:#b84">) +/- (</span><span style="color:#b84">{:.3f}</span><span style="color:#b84">)&#34;</span><span style="font-weight:bold">.</span>format(results<span style="font-weight:bold">.</span>mean(), results<span style="font-weight:bold">.</span>std()))
</span></span></code></pre></div><pre><code>(-0.082) +/- (0.295)
</code></pre>
<p><code>Extra Trees</code></p>
<p>Randomness is introduced to investigate further precision.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>num_trees <span style="font-weight:bold">=</span> <span style="color:#099">100</span>
</span></span><span style="display:flex;"><span>num_features <span style="font-weight:bold">=</span> <span style="color:#099">10</span>
</span></span><span style="display:flex;"><span>model <span style="font-weight:bold">=</span> ExtraTreesClassifier(n_estimators<span style="font-weight:bold">=</span>num_trees, max_features<span style="font-weight:bold">=</span>num_features)
</span></span><span style="display:flex;"><span>results <span style="font-weight:bold">=</span> cross_val_score(model, X_train, y_train, cv<span style="font-weight:bold">=</span>kfold, scoring<span style="font-weight:bold">=</span>scoring, n_jobs<span style="font-weight:bold">=</span>processors)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(<span style="color:#b84">&#34;(</span><span style="color:#b84">{:.3f}</span><span style="color:#b84">) +/- (</span><span style="color:#b84">{:.3f}</span><span style="color:#b84">)&#34;</span><span style="font-weight:bold">.</span>format(results<span style="font-weight:bold">.</span>mean(), results<span style="font-weight:bold">.</span>std()))
</span></span></code></pre></div><pre><code>(0.015) +/- (0.484)
</code></pre>
<p><strong>2. Boosting</strong>
Boosting ensembles a sequence of weak learners for better performance.</p>
<p><code>AdaBoost</code></p>
<p>AdaBoost simply gives the weighted average of results by a series of weak learners, with updating the weight vector in each iteration.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>model <span style="font-weight:bold">=</span> AdaBoostClassifier(n_estimators<span style="font-weight:bold">=</span><span style="color:#099">100</span>, random_state<span style="font-weight:bold">=</span>seed)
</span></span><span style="display:flex;"><span>results <span style="font-weight:bold">=</span> cross_val_score(model, X_train, y_train, cv<span style="font-weight:bold">=</span>kfold, scoring<span style="font-weight:bold">=</span>scoring, n_jobs<span style="font-weight:bold">=</span>processors)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(<span style="color:#b84">&#34;(</span><span style="color:#b84">{:.3f}</span><span style="color:#b84">) +/- (</span><span style="color:#b84">{:.3f}</span><span style="color:#b84">)&#34;</span><span style="font-weight:bold">.</span>format(results<span style="font-weight:bold">.</span>mean(), results<span style="font-weight:bold">.</span>std()))
</span></span></code></pre></div><pre><code>(0.291) +/- (0.387)
</code></pre>
<p><code>Stochastic Gradient Boosting</code>
Gradient Tree Boosting or Gradient Boosted Regression Trees (GBRT) is a generalization of boosting. It uses arbitrary differentiable loss functions so that is more accurate and effective.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>model <span style="font-weight:bold">=</span> GradientBoostingClassifier(n_estimators<span style="font-weight:bold">=</span><span style="color:#099">100</span>, random_state<span style="font-weight:bold">=</span>seed)
</span></span><span style="display:flex;"><span>results <span style="font-weight:bold">=</span> cross_val_score(model, X_train, y_train, cv<span style="font-weight:bold">=</span>kfold, scoring<span style="font-weight:bold">=</span>scoring, n_jobs<span style="font-weight:bold">=</span>processors)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(<span style="color:#b84">&#34;(</span><span style="color:#b84">{:.3f}</span><span style="color:#b84">) +/- (</span><span style="color:#b84">{:.3f}</span><span style="color:#b84">)&#34;</span><span style="font-weight:bold">.</span>format(results<span style="font-weight:bold">.</span>mean(), results<span style="font-weight:bold">.</span>std()))
</span></span></code></pre></div><pre><code>(0.291) +/- (0.387)
</code></pre>
<p><code>Extreme Gradient Boosting</code>
A (usually) more efficient gradient boosting algorithm by Tianqi Chen.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>model <span style="font-weight:bold">=</span> XGBClassifier(n_estimators<span style="font-weight:bold">=</span><span style="color:#099">100</span>, seed<span style="font-weight:bold">=</span>seed)
</span></span><span style="display:flex;"><span>results <span style="font-weight:bold">=</span> cross_val_score(model, X_train, y_train, cv<span style="font-weight:bold">=</span>kfold, scoring<span style="font-weight:bold">=</span>scoring, n_jobs<span style="font-weight:bold">=</span>processors)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(<span style="color:#b84">&#34;(</span><span style="color:#b84">{:.3f}</span><span style="color:#b84">) +/- (</span><span style="color:#b84">{:.3f}</span><span style="color:#b84">)&#34;</span><span style="font-weight:bold">.</span>format(results<span style="font-weight:bold">.</span>mean(), results<span style="font-weight:bold">.</span>std()))
</span></span></code></pre></div><pre><code>(0.189) +/- (0.459)
</code></pre>
<p><strong>3. Hyperparameter tuning</strong></p>
<p>As a matter of fact, hyperparameter tuning can matter a lot here, and thus to actual determine which models are the best, we need to run grid searching and cross validation of the training dataset for the best scores and the model configurations corresponing to them.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>estimator_list <span style="font-weight:bold">=</span> []
</span></span><span style="display:flex;"><span>estimator_name_list <span style="font-weight:bold">=</span> []
</span></span><span style="display:flex;"><span>best_score_list <span style="font-weight:bold">=</span> []
</span></span><span style="display:flex;"><span>best_params_list <span style="font-weight:bold">=</span> []
</span></span></code></pre></div><p><code>Logistic Regression</code></p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>lr_grid <span style="font-weight:bold">=</span> GridSearchCV(estimator <span style="font-weight:bold">=</span> LogisticRegression(random_state<span style="font-weight:bold">=</span>seed),
</span></span><span style="display:flex;"><span>                       param_grid <span style="font-weight:bold">=</span> {<span style="color:#b84">&#39;penalty&#39;</span>: [<span style="color:#b84">&#39;l1&#39;</span>, <span style="color:#b84">&#39;l2&#39;</span>],
</span></span><span style="display:flex;"><span>                                     <span style="color:#b84">&#39;C&#39;</span>: [<span style="color:#099">1e-3</span>, <span style="color:#099">1e-2</span>, <span style="color:#099">1</span>, <span style="color:#099">10</span>, <span style="color:#099">100</span>, <span style="color:#099">1000</span>]},
</span></span><span style="display:flex;"><span>                       cv <span style="font-weight:bold">=</span> kfold, 
</span></span><span style="display:flex;"><span>                       scoring <span style="font-weight:bold">=</span> scoring, 
</span></span><span style="display:flex;"><span>                       n_jobs <span style="font-weight:bold">=</span> processors)
</span></span><span style="display:flex;"><span>lr_grid<span style="font-weight:bold">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(lr_grid<span style="font-weight:bold">.</span>best_score_)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(lr_grid<span style="font-weight:bold">.</span>best_params_)
</span></span><span style="display:flex;"><span>estimator_name_list<span style="font-weight:bold">.</span>append(<span style="color:#b84">&#39;LR&#39;</span>)
</span></span><span style="display:flex;"><span>estimator_list<span style="font-weight:bold">.</span>append(LogisticRegression)
</span></span><span style="display:flex;"><span>best_score_list<span style="font-weight:bold">.</span>append(lr_grid<span style="font-weight:bold">.</span>best_score_)
</span></span><span style="display:flex;"><span>best_params_list<span style="font-weight:bold">.</span>append(lr_grid<span style="font-weight:bold">.</span>best_params_)
</span></span></code></pre></div><pre><code>0.3726887283268559
{'penalty': 'l1', 'C': 1}
</code></pre>
<p><code>Linear Discriminant Analysis</code></p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>lda_grid <span style="font-weight:bold">=</span> GridSearchCV(estimator <span style="font-weight:bold">=</span> LinearDiscriminantAnalysis(),
</span></span><span style="display:flex;"><span>                        param_grid <span style="font-weight:bold">=</span> {<span style="color:#b84">&#39;solver&#39;</span>: [<span style="color:#b84">&#39;svd&#39;</span>,<span style="color:#b84">&#39;lsqr&#39;</span>],
</span></span><span style="display:flex;"><span>                                      <span style="color:#b84">&#39;n_components&#39;</span>: [<span style="font-weight:bold">None</span>, <span style="color:#099">2</span>, <span style="color:#099">5</span>, <span style="color:#099">10</span>, <span style="color:#099">20</span>]}, 
</span></span><span style="display:flex;"><span>                        cv <span style="font-weight:bold">=</span> kfold,
</span></span><span style="display:flex;"><span>                        scoring <span style="font-weight:bold">=</span> scoring, 
</span></span><span style="display:flex;"><span>                        n_jobs <span style="font-weight:bold">=</span> processors)
</span></span><span style="display:flex;"><span>lda_grid<span style="font-weight:bold">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(lda_grid<span style="font-weight:bold">.</span>best_score_)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(lda_grid<span style="font-weight:bold">.</span>best_params_)
</span></span><span style="display:flex;"><span>estimator_name_list<span style="font-weight:bold">.</span>append(<span style="color:#b84">&#39;LDA&#39;</span>)
</span></span><span style="display:flex;"><span>estimator_list<span style="font-weight:bold">.</span>append(LinearDiscriminantAnalysis)
</span></span><span style="display:flex;"><span>best_score_list<span style="font-weight:bold">.</span>append(lda_grid<span style="font-weight:bold">.</span>best_score_)
</span></span><span style="display:flex;"><span>best_params_list<span style="font-weight:bold">.</span>append(lda_grid<span style="font-weight:bold">.</span>best_params_)
</span></span></code></pre></div><pre><code>0.5622889460982887
{'n_components': None, 'solver': 'svd'}
</code></pre>
<p><code>Decision Tree</code></p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>dt_grid <span style="font-weight:bold">=</span> GridSearchCV(estimator <span style="font-weight:bold">=</span> DecisionTreeClassifier(random_state<span style="font-weight:bold">=</span>seed),
</span></span><span style="display:flex;"><span>                       param_grid <span style="font-weight:bold">=</span> {<span style="color:#b84">&#39;criterion&#39;</span>: [<span style="color:#b84">&#39;gini&#39;</span>,<span style="color:#b84">&#39;entropy&#39;</span>],
</span></span><span style="display:flex;"><span>                                     <span style="color:#b84">&#39;max_depth&#39;</span>: [<span style="font-weight:bold">None</span>, <span style="color:#099">2</span>, <span style="color:#099">5</span>, <span style="color:#099">10</span>], 
</span></span><span style="display:flex;"><span>                                     <span style="color:#b84">&#39;max_features&#39;</span>: [<span style="font-weight:bold">None</span>,<span style="color:#b84">&#39;auto&#39;</span>,<span style="color:#099">20</span>]},
</span></span><span style="display:flex;"><span>                       cv <span style="font-weight:bold">=</span> kfold,
</span></span><span style="display:flex;"><span>                       scoring <span style="font-weight:bold">=</span> scoring, 
</span></span><span style="display:flex;"><span>                       n_jobs <span style="font-weight:bold">=</span> processors)
</span></span><span style="display:flex;"><span>dt_grid<span style="font-weight:bold">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(dt_grid<span style="font-weight:bold">.</span>best_score_)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(dt_grid<span style="font-weight:bold">.</span>best_params_)
</span></span><span style="display:flex;"><span>estimator_name_list<span style="font-weight:bold">.</span>append(<span style="color:#b84">&#39;DT&#39;</span>)
</span></span><span style="display:flex;"><span>estimator_list<span style="font-weight:bold">.</span>append(DecisionTreeClassifier)
</span></span><span style="display:flex;"><span>best_score_list<span style="font-weight:bold">.</span>append(dt_grid<span style="font-weight:bold">.</span>best_score_)
</span></span><span style="display:flex;"><span>best_params_list<span style="font-weight:bold">.</span>append(dt_grid<span style="font-weight:bold">.</span>best_params_)
</span></span></code></pre></div><pre><code>0.34852862485121184
{'criterion': 'gini', 'max_depth': None, 'max_features': None}
</code></pre>
<p><code>K-Nearest Neighbors</code></p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>knn_grid <span style="font-weight:bold">=</span> GridSearchCV(estimator <span style="font-weight:bold">=</span> KNeighborsClassifier(),
</span></span><span style="display:flex;"><span>                        param_grid <span style="font-weight:bold">=</span> {<span style="color:#b84">&#39;n_neighbors&#39;</span>: [<span style="color:#099">5</span>, <span style="color:#099">10</span>, <span style="color:#099">25</span>],
</span></span><span style="display:flex;"><span>                                      <span style="color:#b84">&#39;algorithm&#39;</span>: [<span style="color:#b84">&#39;ball_tree&#39;</span>],
</span></span><span style="display:flex;"><span>                                      <span style="color:#b84">&#39;leaf_size&#39;</span>: [<span style="color:#099">2</span>, <span style="color:#099">3</span>, <span style="color:#099">5</span>],
</span></span><span style="display:flex;"><span>                                      <span style="color:#b84">&#39;p&#39;</span>: [<span style="color:#099">1</span>]}, 
</span></span><span style="display:flex;"><span>                        cv <span style="font-weight:bold">=</span> kfold, 
</span></span><span style="display:flex;"><span>                        scoring <span style="font-weight:bold">=</span> scoring, 
</span></span><span style="display:flex;"><span>                        n_jobs <span style="font-weight:bold">=</span> processors)
</span></span><span style="display:flex;"><span>knn_grid<span style="font-weight:bold">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(knn_grid<span style="font-weight:bold">.</span>best_score_)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(knn_grid<span style="font-weight:bold">.</span>best_params_)
</span></span><span style="display:flex;"><span>estimator_name_list<span style="font-weight:bold">.</span>append(<span style="color:#b84">&#39;KNN&#39;</span>)
</span></span><span style="display:flex;"><span>estimator_list<span style="font-weight:bold">.</span>append(KNeighborsClassifier)
</span></span><span style="display:flex;"><span>best_score_list<span style="font-weight:bold">.</span>append(knn_grid<span style="font-weight:bold">.</span>best_score_)
</span></span><span style="display:flex;"><span>best_params_list<span style="font-weight:bold">.</span>append(knn_grid<span style="font-weight:bold">.</span>best_params_)
</span></span></code></pre></div><pre><code>0.30539557863515217
{'leaf_size': 2, 'algorithm': 'ball_tree', 'n_neighbors': 5, 'p': 1}
</code></pre>
<p><code>Random Forest</code></p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>rf_grid <span style="font-weight:bold">=</span> GridSearchCV(estimator <span style="font-weight:bold">=</span> RandomForestClassifier(warm_start<span style="font-weight:bold">=</span><span style="font-weight:bold">True</span>, random_state<span style="font-weight:bold">=</span>seed),
</span></span><span style="display:flex;"><span>                       param_grid <span style="font-weight:bold">=</span> {<span style="color:#b84">&#39;n_estimators&#39;</span>: [<span style="color:#099">100</span>, <span style="color:#099">200</span>],
</span></span><span style="display:flex;"><span>                                     <span style="color:#b84">&#39;criterion&#39;</span>: [<span style="color:#b84">&#39;gini&#39;</span>,<span style="color:#b84">&#39;entropy&#39;</span>],
</span></span><span style="display:flex;"><span>                                     <span style="color:#b84">&#39;max_features&#39;</span>: [<span style="font-weight:bold">None</span>, <span style="color:#099">20</span>],
</span></span><span style="display:flex;"><span>                                     <span style="color:#b84">&#39;max_depth&#39;</span>: [<span style="color:#099">3</span>, <span style="color:#099">5</span>, <span style="color:#099">10</span>],
</span></span><span style="display:flex;"><span>                                     <span style="color:#b84">&#39;bootstrap&#39;</span>: [<span style="font-weight:bold">True</span>]}, 
</span></span><span style="display:flex;"><span>                       cv <span style="font-weight:bold">=</span> kfold, 
</span></span><span style="display:flex;"><span>                       scoring <span style="font-weight:bold">=</span> scoring, 
</span></span><span style="display:flex;"><span>                       n_jobs <span style="font-weight:bold">=</span> processors)
</span></span><span style="display:flex;"><span>rf_grid<span style="font-weight:bold">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(rf_grid<span style="font-weight:bold">.</span>best_score_)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(rf_grid<span style="font-weight:bold">.</span>best_params_)
</span></span><span style="display:flex;"><span>estimator_name_list<span style="font-weight:bold">.</span>append(<span style="color:#b84">&#39;RF&#39;</span>)
</span></span><span style="display:flex;"><span>estimator_list<span style="font-weight:bold">.</span>append(RandomForestClassifier)
</span></span><span style="display:flex;"><span>best_score_list<span style="font-weight:bold">.</span>append(rf_grid<span style="font-weight:bold">.</span>best_score_)
</span></span><span style="display:flex;"><span>best_params_list<span style="font-weight:bold">.</span>append(rf_grid<span style="font-weight:bold">.</span>best_params_)
</span></span></code></pre></div><pre><code>0.30113313283861066
{'bootstrap': True, 'max_depth': 5, 'n_estimators': 100, 'max_features': None, 'criterion': 'entropy'}
</code></pre>
<p><code>Extra Trees</code></p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>ext_grid <span style="font-weight:bold">=</span> GridSearchCV(estimator <span style="font-weight:bold">=</span> ExtraTreesClassifier(warm_start<span style="font-weight:bold">=</span><span style="font-weight:bold">True</span>, random_state<span style="font-weight:bold">=</span>seed),
</span></span><span style="display:flex;"><span>                        param_grid <span style="font-weight:bold">=</span> {<span style="color:#b84">&#39;n_estimators&#39;</span>: [<span style="color:#099">100</span>, <span style="color:#099">200</span>],
</span></span><span style="display:flex;"><span>                                      <span style="color:#b84">&#39;criterion&#39;</span>: [<span style="color:#b84">&#39;gini&#39;</span>, <span style="color:#b84">&#39;entropy&#39;</span>],
</span></span><span style="display:flex;"><span>                                      <span style="color:#b84">&#39;max_features&#39;</span>: [<span style="font-weight:bold">None</span>, <span style="color:#099">20</span>],
</span></span><span style="display:flex;"><span>                                      <span style="color:#b84">&#39;max_depth&#39;</span>: [<span style="color:#099">3</span>, <span style="color:#099">5</span>, <span style="color:#099">10</span>],
</span></span><span style="display:flex;"><span>                                      <span style="color:#b84">&#39;bootstrap&#39;</span>: [<span style="font-weight:bold">True</span>]}, 
</span></span><span style="display:flex;"><span>                        cv <span style="font-weight:bold">=</span> kfold, 
</span></span><span style="display:flex;"><span>                        scoring <span style="font-weight:bold">=</span> scoring, 
</span></span><span style="display:flex;"><span>                        n_jobs <span style="font-weight:bold">=</span> processors)
</span></span><span style="display:flex;"><span>ext_grid<span style="font-weight:bold">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(ext_grid<span style="font-weight:bold">.</span>best_score_)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(ext_grid<span style="font-weight:bold">.</span>best_params_)
</span></span><span style="display:flex;"><span>estimator_name_list<span style="font-weight:bold">.</span>append(<span style="color:#b84">&#39;EXT&#39;</span>)
</span></span><span style="display:flex;"><span>estimator_list<span style="font-weight:bold">.</span>append(ExtraTreesClassifier)
</span></span><span style="display:flex;"><span>best_score_list<span style="font-weight:bold">.</span>append(ext_grid<span style="font-weight:bold">.</span>best_score_)
</span></span><span style="display:flex;"><span>best_params_list<span style="font-weight:bold">.</span>append(ext_grid<span style="font-weight:bold">.</span>best_params_)
</span></span></code></pre></div><pre><code>0.2601389681995039
{'bootstrap': True, 'max_depth': 10, 'n_estimators': 100, 'max_features': 20, 'criterion': 'entropy'}
</code></pre>
<p><code>AdaBoost</code></p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>ada_grid <span style="font-weight:bold">=</span> GridSearchCV(estimator <span style="font-weight:bold">=</span> AdaBoostClassifier(random_state<span style="font-weight:bold">=</span>seed),
</span></span><span style="display:flex;"><span>                        param_grid <span style="font-weight:bold">=</span> {<span style="color:#b84">&#39;algorithm&#39;</span>: [<span style="color:#b84">&#39;SAMME&#39;</span>, <span style="color:#b84">&#39;SAMME.R&#39;</span>],
</span></span><span style="display:flex;"><span>                                      <span style="color:#b84">&#39;n_estimators&#39;</span>: [<span style="color:#099">100</span>, <span style="color:#099">200</span>],
</span></span><span style="display:flex;"><span>                                      <span style="color:#b84">&#39;learning_rate&#39;</span>: [<span style="color:#099">1e-2</span>, <span style="color:#099">1e-1</span>]}, 
</span></span><span style="display:flex;"><span>                        cv <span style="font-weight:bold">=</span> kfold, 
</span></span><span style="display:flex;"><span>                        scoring <span style="font-weight:bold">=</span> scoring, 
</span></span><span style="display:flex;"><span>                        n_jobs <span style="font-weight:bold">=</span> processors)
</span></span><span style="display:flex;"><span>ada_grid<span style="font-weight:bold">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(ada_grid<span style="font-weight:bold">.</span>best_score_)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(ada_grid<span style="font-weight:bold">.</span>best_params_)
</span></span><span style="display:flex;"><span>estimator_name_list<span style="font-weight:bold">.</span>append(<span style="color:#b84">&#39;ADA&#39;</span>)
</span></span><span style="display:flex;"><span>estimator_list<span style="font-weight:bold">.</span>append(AdaBoostClassifier)
</span></span><span style="display:flex;"><span>best_score_list<span style="font-weight:bold">.</span>append(ada_grid<span style="font-weight:bold">.</span>best_score_)
</span></span><span style="display:flex;"><span>best_params_list<span style="font-weight:bold">.</span>append(ada_grid<span style="font-weight:bold">.</span>best_params_)
</span></span></code></pre></div><pre><code>0.30113313283861066
{'n_estimators': 200, 'algorithm': 'SAMME', 'learning_rate': 0.1}
</code></pre>
<p><code>Gradient Boosting</code></p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>gbm_grid <span style="font-weight:bold">=</span> GridSearchCV(estimator <span style="font-weight:bold">=</span> GradientBoostingClassifier(warm_start<span style="font-weight:bold">=</span><span style="font-weight:bold">True</span>, random_state<span style="font-weight:bold">=</span>seed),
</span></span><span style="display:flex;"><span>                        param_grid <span style="font-weight:bold">=</span> {<span style="color:#b84">&#39;n_estimators&#39;</span>: [<span style="color:#099">100</span>, <span style="color:#099">200</span>],
</span></span><span style="display:flex;"><span>                                      <span style="color:#b84">&#39;max_depth&#39;</span>: [<span style="color:#099">3</span>, <span style="color:#099">5</span>, <span style="color:#099">10</span>],
</span></span><span style="display:flex;"><span>                                      <span style="color:#b84">&#39;max_features&#39;</span>: [<span style="font-weight:bold">None</span>, <span style="color:#099">20</span>],
</span></span><span style="display:flex;"><span>                                      <span style="color:#b84">&#39;learning_rate&#39;</span>: [<span style="color:#099">1e-2</span>, <span style="color:#099">1e-1</span>]}, 
</span></span><span style="display:flex;"><span>                        cv <span style="font-weight:bold">=</span> kfold, 
</span></span><span style="display:flex;"><span>                        scoring <span style="font-weight:bold">=</span> scoring, 
</span></span><span style="display:flex;"><span>                        n_jobs <span style="font-weight:bold">=</span> processors)
</span></span><span style="display:flex;"><span>gbm_grid<span style="font-weight:bold">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(gbm_grid<span style="font-weight:bold">.</span>best_score_)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(gbm_grid<span style="font-weight:bold">.</span>best_params_)
</span></span><span style="display:flex;"><span>estimator_name_list<span style="font-weight:bold">.</span>append(<span style="color:#b84">&#39;GBM&#39;</span>)
</span></span><span style="display:flex;"><span>estimator_list<span style="font-weight:bold">.</span>append(GradientBoostingClassifier)
</span></span><span style="display:flex;"><span>best_score_list<span style="font-weight:bold">.</span>append(gbm_grid<span style="font-weight:bold">.</span>best_score_)
</span></span><span style="display:flex;"><span>best_params_list<span style="font-weight:bold">.</span>append(gbm_grid<span style="font-weight:bold">.</span>best_params_)
</span></span></code></pre></div><pre><code>0.4376019643046027
{'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'max_features': None}
</code></pre>
<p><code>Extreme Gradient Boosting</code></p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>xgb_grid <span style="font-weight:bold">=</span> GridSearchCV(estimator <span style="font-weight:bold">=</span> XGBClassifier(nthread<span style="font-weight:bold">=</span><span style="color:#099">1</span>, seed<span style="font-weight:bold">=</span>seed),
</span></span><span style="display:flex;"><span>                        param_grid <span style="font-weight:bold">=</span> {<span style="color:#b84">&#39;n_estimators&#39;</span>: [<span style="color:#099">100</span>, <span style="color:#099">200</span>],
</span></span><span style="display:flex;"><span>                                      <span style="color:#b84">&#39;max_depth&#39;</span>: [<span style="color:#099">3</span>, <span style="color:#099">5</span>, <span style="color:#099">10</span>],
</span></span><span style="display:flex;"><span>                                      <span style="color:#b84">&#39;min_child_weight&#39;</span>: [<span style="color:#099">1</span>, <span style="color:#099">5</span>],
</span></span><span style="display:flex;"><span>                                      <span style="color:#b84">&#39;gamma&#39;</span>: [<span style="color:#099">0</span>, <span style="color:#099">0.1</span>, <span style="color:#099">1</span>, <span style="color:#099">10</span>],
</span></span><span style="display:flex;"><span>                                      <span style="color:#b84">&#39;learning_rate&#39;</span>: [<span style="color:#099">1e-2</span>, <span style="color:#099">1e-1</span>]}, 
</span></span><span style="display:flex;"><span>                        cv <span style="font-weight:bold">=</span> kfold, 
</span></span><span style="display:flex;"><span>                        scoring <span style="font-weight:bold">=</span> scoring, 
</span></span><span style="display:flex;"><span>                        n_jobs <span style="font-weight:bold">=</span> processors)
</span></span><span style="display:flex;"><span>xgb_grid<span style="font-weight:bold">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(xgb_grid<span style="font-weight:bold">.</span>best_score_)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(xgb_grid<span style="font-weight:bold">.</span>best_params_)
</span></span><span style="display:flex;"><span>estimator_name_list<span style="font-weight:bold">.</span>append(<span style="color:#b84">&#39;XGB&#39;</span>)
</span></span><span style="display:flex;"><span>estimator_list<span style="font-weight:bold">.</span>append(XGBClassifier)
</span></span><span style="display:flex;"><span>best_score_list<span style="font-weight:bold">.</span>append(xgb_grid<span style="font-weight:bold">.</span>best_score_)
</span></span><span style="display:flex;"><span>best_params_list<span style="font-weight:bold">.</span>append(xgb_grid<span style="font-weight:bold">.</span>best_params_)
</span></span></code></pre></div><pre><code>0.556562176665963
{'gamma': 0, 'min_child_weight': 1, 'max_depth': 5, 'learning_rate': 0.01, 'n_estimators': 200}
</code></pre>
<p><code>Support Vector Classification</code></p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>svc_grid <span style="font-weight:bold">=</span> GridSearchCV(estimator <span style="font-weight:bold">=</span> SVC(probability<span style="font-weight:bold">=</span><span style="font-weight:bold">True</span>, class_weight<span style="font-weight:bold">=</span><span style="color:#b84">&#39;balanced&#39;</span>),
</span></span><span style="display:flex;"><span>                        param_grid <span style="font-weight:bold">=</span> {<span style="color:#b84">&#39;C&#39;</span>: [<span style="color:#099">1e-3</span>, <span style="color:#099">1e-2</span>, <span style="color:#099">1e-1</span>, <span style="color:#099">1</span>, <span style="color:#099">10</span>],
</span></span><span style="display:flex;"><span>                                      <span style="color:#b84">&#39;gamma&#39;</span>: [<span style="color:#099">1e-2</span>, <span style="color:#099">1e-1</span>, <span style="color:#099">1</span>]},
</span></span><span style="display:flex;"><span>                        cv <span style="font-weight:bold">=</span> kfold, 
</span></span><span style="display:flex;"><span>                        scoring <span style="font-weight:bold">=</span> scoring, 
</span></span><span style="display:flex;"><span>                        n_jobs <span style="font-weight:bold">=</span> processors)
</span></span><span style="display:flex;"><span>svc_grid<span style="font-weight:bold">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(svc_grid<span style="font-weight:bold">.</span>best_score_)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(svc_grid<span style="font-weight:bold">.</span>best_params_)
</span></span><span style="display:flex;"><span>estimator_name_list<span style="font-weight:bold">.</span>append(<span style="color:#b84">&#39;SVC&#39;</span>)
</span></span><span style="display:flex;"><span>estimator_list<span style="font-weight:bold">.</span>append(SVC)
</span></span><span style="display:flex;"><span>best_score_list<span style="font-weight:bold">.</span>append(svc_grid<span style="font-weight:bold">.</span>best_score_)
</span></span><span style="display:flex;"><span>best_params_list<span style="font-weight:bold">.</span>append(svc_grid<span style="font-weight:bold">.</span>best_params_)
</span></span></code></pre></div><pre><code>0.3422932503617775
{'gamma': 0.01, 'C': 0.1}
</code></pre>
<p><strong>4. Voting ensemble</strong></p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>best_score_list_rounded <span style="font-weight:bold">=</span> [<span style="color:#999">round</span>(s,<span style="color:#099">3</span>) <span style="font-weight:bold">for</span> s <span style="font-weight:bold">in</span> best_score_list]
</span></span><span style="display:flex;"><span>seq <span style="font-weight:bold">=</span> <span style="color:#999">sorted</span>(best_score_list, reverse<span style="font-weight:bold">=</span><span style="font-weight:bold">True</span>)
</span></span><span style="display:flex;"><span>rank <span style="font-weight:bold">=</span> [seq<span style="font-weight:bold">.</span>index(s) <span style="font-weight:bold">for</span> s <span style="font-weight:bold">in</span> best_score_list]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>res <span style="font-weight:bold">=</span> pd<span style="font-weight:bold">.</span>DataFrame({<span style="color:#b84">&#39;model&#39;</span>:estimator_name_list,
</span></span><span style="display:flex;"><span>                    <span style="color:#b84">&#39;score&#39;</span>:best_score_list_rounded,
</span></span><span style="display:flex;"><span>                    <span style="color:#b84">&#39;rank&#39;</span>:rank},columns<span style="font-weight:bold">=</span>[<span style="color:#b84">&#39;model&#39;</span>,<span style="color:#b84">&#39;score&#39;</span>,<span style="color:#b84">&#39;rank&#39;</span>])<span style="font-weight:bold">.</span>set_index(<span style="color:#b84">&#39;model&#39;</span>)
</span></span><span style="display:flex;"><span>res[<span style="color:#b84">&#39;rank&#39;</span>] <span style="font-weight:bold">=</span> res[<span style="color:#b84">&#39;rank&#39;</span>]<span style="font-weight:bold">.</span>astype(<span style="color:#b84">&#39;object&#39;</span>)
</span></span><span style="display:flex;"><span>res<span style="font-weight:bold">.</span>T
</span></span></code></pre></div><table>
<thead>
<tr>
<th style="text-align:center">model</th>
<th style="text-align:center">LR</th>
<th style="text-align:center">LDA</th>
<th style="text-align:center">DT</th>
<th style="text-align:center">KNN</th>
<th style="text-align:center">RF</th>
<th style="text-align:center">EXT</th>
<th style="text-align:center">ADA</th>
<th style="text-align:center">GBM</th>
<th style="text-align:center">XGB</th>
<th style="text-align:center">SVC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">score</td>
<td style="text-align:center">0.373</td>
<td style="text-align:center">0.562</td>
<td style="text-align:center">0.349</td>
<td style="text-align:center">0.305</td>
<td style="text-align:center">0.301</td>
<td style="text-align:center">0.26</td>
<td style="text-align:center">0.301</td>
<td style="text-align:center">0.438</td>
<td style="text-align:center">0.557</td>
<td style="text-align:center">0.342</td>
</tr>
<tr>
<td style="text-align:center">rank</td>
<td style="text-align:center">3</td>
<td style="text-align:center">0</td>
<td style="text-align:center">4</td>
<td style="text-align:center">6</td>
<td style="text-align:center">7</td>
<td style="text-align:center">9</td>
<td style="text-align:center">7</td>
<td style="text-align:center">2</td>
<td style="text-align:center">1</td>
<td style="text-align:center">5</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Create sub models</span>
</span></span><span style="display:flex;"><span>estimators <span style="font-weight:bold">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Choose models with larger logloss scores</span>
</span></span><span style="display:flex;"><span><span style="font-weight:bold">for</span> i <span style="font-weight:bold">in</span> <span style="color:#999">range</span>(<span style="color:#099">10</span>):  <span style="color:#998;font-style:italic"># in total 10 models</span>
</span></span><span style="display:flex;"><span>    <span style="font-weight:bold">if</span> rank[i] <span style="font-weight:bold">&lt;</span> <span style="color:#099">5</span>:  <span style="color:#998;font-style:italic"># if in top 5</span>
</span></span><span style="display:flex;"><span>        estimators<span style="font-weight:bold">.</span>append((estimator_name_list[i], estimator_list[i](<span style="font-weight:bold">**</span>best_params_list[i])))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Create the ensemble model</span>
</span></span><span style="display:flex;"><span>ensemble <span style="font-weight:bold">=</span> VotingClassifier(estimators, voting<span style="font-weight:bold">=</span><span style="color:#b84">&#39;hard&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>results <span style="font-weight:bold">=</span> cross_val_score(ensemble, X_train, y_train, cv<span style="font-weight:bold">=</span>kfold, scoring<span style="font-weight:bold">=</span>scoring,n_jobs<span style="font-weight:bold">=</span>processors)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(<span style="color:#b84">&#34;(</span><span style="color:#b84">{:.3f}</span><span style="color:#b84">) +/- (</span><span style="color:#b84">{:.3f}</span><span style="color:#b84">)&#34;</span><span style="font-weight:bold">.</span>format(results<span style="font-weight:bold">.</span>mean(), results<span style="font-weight:bold">.</span>std()))
</span></span></code></pre></div><pre><code>(0.516) +/- (0.200)
</code></pre>
<p>It is clear that the ensemble model further enhanced the performance of the seperate models. Now we try to make actual predictions and see if the results are robust.</p>
<p><strong>5. Make predictions</strong></p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>model <span style="font-weight:bold">=</span> ensemble
</span></span><span style="display:flex;"><span>model<span style="font-weight:bold">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span>y_pred <span style="font-weight:bold">=</span> model<span style="font-weight:bold">.</span>predict(X_test)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#999">print</span>(<span style="color:#b84">&#39;Unbalance of the data: </span><span style="color:#b84">{:.3f}</span><span style="color:#b84">&#39;</span><span style="font-weight:bold">.</span>format(unbalance))
</span></span></code></pre></div><pre><code>Unbalance of the data: 0.533
</code></pre>
<p>Now apart from the utility, we can check our prediction based on some other metrics, e.g.:</p>
<p><code>Accuracy</code></p>
<p>which is defined by</p>
<div>
$$
\begin{align*}
\text{Accuracy}
&=\frac{|\text{True Positive}|+|\text{True Negative}|}{|\text{Total Polulation}|}\newline
&=\frac{|\text{True Positive}|+|\text{True Negative}|}{|\text{TP}|+|\text{FP}|+|\text{TN}|+|\text{FN}|}\newline
&=\frac{1}{n}\sum_{i=1}^n\mathbb{1}(\hat{y}_i=y_i)
\end{align*}
$$
</div>
<p>and should be bounded within <code>$[0,1]$</code>, where <code>$1$</code> indicates perfect prediction. This is also called total accuracy, and it calculates the percentage of a right guess.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>ac <span style="font-weight:bold">=</span> accuracy_score(y_test, y_pred)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(<span style="color:#b84">&#39;Accuracy: </span><span style="color:#b84">{:.3f}</span><span style="color:#b84">&#39;</span><span style="font-weight:bold">.</span>format(ac))  <span style="color:#998;font-style:italic"># (TP+TN) / (TP+FP+TN+FN)</span>
</span></span></code></pre></div><pre><code>Accuracy: 0.667
</code></pre>
<p><code>Precision</code></p>
<p>which is defined by</p>
<div>
$$
\begin{align}
\text{Precision} &= 
\frac{|\text{True Positive}|}{|\text{True Positive}|+|\text{False Positive}|}\newline &=
\frac{\sum_{i=1}^n\mathbb{1}(\hat{y}_i=1\mid y_i=1)}{\sum_{i=1}^n[\mathbb{1}(\hat{y}_i=1\mid y_i=1)+\mathbb{1}(\hat{y}_i=1\mid y_i=0)]}.
\end{align}
$$
</div>
<p>Similar as accuracy, this is also bounded and indicates perfect prediction when the value is 1. However, precision gives intuition about the percentage of correct guesses among all your guesses, so in this case the probability that your actual transaction is in the right direction.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>pc <span style="font-weight:bold">=</span> precision_score(y_test, y_pred)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(<span style="color:#b84">&#39;Precision: </span><span style="color:#b84">{:.3f}</span><span style="color:#b84">&#39;</span><span style="font-weight:bold">.</span>format(pc))  <span style="color:#998;font-style:italic"># TP / (TP+FP)</span>
</span></span></code></pre></div><pre><code>Precision: 0.818
</code></pre>
<p><code>Recall</code></p>
<p>which is defined by</p>
<div>
$$
\begin{align}
\text{Recall} &= 
\frac{|\text{True Positive}|}{|\text{True Positive}|+|\text{False Negative}|}\\&=
\frac{\sum_{i=1}^n\mathbb{1}(\hat{y}_i=1\mid y_i=1)}{\sum_{i=1}^n[\mathbb{1}(\hat{y}_i=1\mid y_i=1)+\mathbb{1}(\hat{y}_i=0\mid y_i=1)]}.
\end{align}
$$
</div>
<p>Recall is also bounded and indicates perfect prediction when it&rsquo;s 1, but different from precision, it gives intuition about the percentage of actual signals being predicted, i.e. in this case, the probability that you catch an actual appreciation.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>re <span style="font-weight:bold">=</span> recall_score(y_test, y_pred)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(<span style="color:#b84">&#39;Recall: </span><span style="color:#b84">{:.3f}</span><span style="color:#b84">&#39;</span><span style="font-weight:bold">.</span>format(re))  <span style="color:#998;font-style:italic"># TP / (TP+FN)</span>
</span></span></code></pre></div><pre><code>Recall: 0.750
</code></pre>
<p>Although not included in this notebook, we think it is very important and encouraging to mention what these scores mean, compared with when other scoring functions are used in grid searching. As a matter of fact, the average accuracy given by the ensemble model using accuracy or log loss directly is much lower than these figures above. In general, the model prediction accuracy has been improved from 15% - 25% to 60% - 80%, i.e. 2 to 5 times. The effect of introducing this utility-like scoring function for hyperparameter tuning is substantial, though needs further theoretical proofs, of course.</p>
<p>Lastly, let&rsquo;s check this utility value for the testing dataset.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>cs <span style="font-weight:bold">=</span> custom_score(y_test, y_pred)
</span></span><span style="display:flex;"><span><span style="color:#999">print</span>(<span style="color:#b84">&#39;Utility: </span><span style="color:#b84">{:.3f}</span><span style="color:#b84">&#39;</span><span style="font-weight:bold">.</span>format(cs))
</span></span></code></pre></div><pre><code>Utility: 0.287
</code></pre>
<p>which is thus robust (even higher, in fact) out of sample.</p>
<h1 id="conclusion-and-prospective-improvements">Conclusion and Prospective Improvements</h1>
<p>The ensemble model I&rsquo;ve just shown above is quite naive, I would say, and is far from &ldquo;good&rdquo;. The metric is more or less still quite arbitrary and the algorithm is rather slow (so I set window length to 3 from 60 at first, which was intended to train a model on data of 5 years), and thus on an unprofessional platform like Ricequant we&rsquo;re not allowed to run through the whole market and search for a best portfolio &ndash; the most predictable ones. In the backtest strategy I implimented based on this research paper, I only chose 5 stocks arbitrarily from the index components of 000050.XSHG, and this can have an unpredictable downside effect on the model performance. A more desirable idea would be to set up a local backtest environment and implement this process with the help of GPU and faster languages like C++. Moreover, overfitting is possible, very possible, and thus whether it will make a good strategy needs much more work of validation.</p>
<hr>
<h1 id="references">References</h1>
<ul>
<li>Arrow, K. J. (1965). &ldquo;Aspects of the Theory of Risk Bearing&rdquo;. The Theory of Risk Aversion. Helsinki: Yrjo Jahnssonin Saatio. Reprinted in: Essays in the Theory of Risk Bearing, Markham Publ. Co., Chicago, 1971, 90109.</li>
<li>Pratt, J. W. (1964). &ldquo;Risk Aversion in the Small and in the Large&rdquo;. Econometrica. 32 (12): 122136.</li>
<li>Kahneman, Daniel; Tversky, Amos (1979). &ldquo;Prospect theory: An analysis of decision under risk&rdquo;. Econometrica: Journal of the econometric society. 47 (2): 263291.</li>
<li>Chen, T., &amp; Guestrin, C. (2016). &ldquo;Xgboost: A scalable tree boosting system&rdquo;. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining (pp. 785-794). ACM.</li>
</ul>


<script>
  var unfocusableElems = document.querySelectorAll('pre');
  unfocusableElems.forEach(function (el) { el.setAttribute("tabindex", "-1"); });
  var unfocusableElems = document.querySelectorAll('iframe');
  unfocusableElems.forEach(function (el) { el.setAttribute("tabindex", "-1"); });
</script>

<footer>
  
<br><br>
<nav class="post-nav">
  <span class="nav-prev">&larr; <a href="/blog/how-to-build-cpp-with-stdin/">How to Build C&#43;&#43; Projects with STDIN in Sublime Text 3</a></span>
  <span class="nav-next"><a href="/blog/network-visualization/">Network Visualization: Chinese Railway Transport from 1992 to 2011</a> &rarr;</span>
</nav>

<script type="text/javascript">
document.addEventListener('keyup', function(e) {
  if (e.target.nodeName.toUpperCase() != 'BODY') return;
  var url = false;
  if (e.which == 37) {  
    
    url = '\/blog\/how-to-build-cpp-with-stdin\/';
    
  } else if (e.which == 39) {  
    
    url = '\/blog\/network-visualization\/';
    
  }
  if (url) window.location = url;
});
</script>



<script src="https://giscus.app/client.js" data-repo="allenfrostline/allenfrostline.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3NzEzOTkxNg==" data-category="General" data-category-id="DIC_kwDOBJkPzM4CbgIQ"
        data-mapping="pathname" data-strict="0" data-reactions-enabled="0" data-emit-metadata="0"
        data-input-position="bottom" data-theme="light" data-lang="en" data-loading="lazy" crossorigin="anonymous"
        async>
        </script>



<script async src="/js/alt-title.js"></script>

<script async src="/js/center-img.js"></script>

<script async src="/js/external-link.js"></script>

<script async src="/js/fix-footnote.js"></script>

<script async src="/js/header-link.js"></script>

<script async src="/js/load-typekit.js"></script>

<script async src="/js/math-code.js"></script>

<script async src="/js/mermaid.min.js"></script>

<script async src="/js/right-quote.js"></script>


<script src="/js/math-code.js"></script>

  
  
  
  
</footer>
</article>
</body>

</html>
