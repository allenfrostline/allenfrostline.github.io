<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>classification on Allen&#39;s Whiteboard</title>
    <link>https://allenfrostline.com/tags/classification/</link>
    <description>Recent content in classification on Allen&#39;s Whiteboard</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 12 Mar 2017 20:45:00 +0000</lastBuildDate>
    <atom:link href="https://allenfrostline.com/tags/classification/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Strategy: XGBoost Classification &#43; Technical Indicators</title>
      <link>https://allenfrostline.com/blog/strategy-xgboost-technical-indicators/</link>
      <pubDate>Sun, 12 Mar 2017 20:45:00 +0000</pubDate>
      <guid>https://allenfrostline.com/blog/strategy-xgboost-technical-indicators/</guid>
      <description>On many Kaggle public leaderboards, you will see an algorithm called &amp;ldquo;XGBoost&amp;rdquo;, or &amp;ldquo;Xtreme Gradient Boosting&amp;rdquo;. This is an optimized algorithm under the Gradient Boosting framework. The key difference between the normal boosting models and XGBoost, as far as I can see, is that XGBoost allows for parallel processing while keeps the accuracy on its performance. This dramatically makes large-scale machine learning algorithms (since boosting is essentially an ensemble of several weak learning algorithms) possible on personal computers (even on my MBP 2015), and further empowers people like me to apply them onto more practical scenarios, e.</description>
    </item>
  </channel>
</rss>
